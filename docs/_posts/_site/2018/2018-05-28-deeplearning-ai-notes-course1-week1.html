<p>==提示：<strong>为方便阅读，《deeplearning.ai深度学习笔记》系列博文已统一整理到 <a href="http://dl-notes.imshuai.com">http://dl-notes.imshuai.com</a></strong>==</p>

<p>Andrew Ng去年离开百度后，再次投身到了人工智能领域的大众教育上，并推出了<a href="http://www.deeplearning.ai">deeplearning.ai</a>网站，旨在建立深度学习教育品牌。不过目前课程主要还是托管在Coursera上，deeplearning.ai本身只不过就是一个 Landing page。</p>

<p>学完了机器学习（<a href="/coursera-machine-learning-review/">Coursera Machine Learning机器学习课程总结</a>），我要快马加鞭把深度学习也搞一遍。</p>

<p>之前Machine Learning的笔记都是记在自己的Evernote里面；既然要记录，不然直接放到博客，互相交流岂不更好。</p>

<p>目前在Coursera上以Specialization的形式出现，一共包含5个Coursera：</p>
<ul>
  <li>Course 1 Neural Networks and Deep Learning</li>
  <li>Course 2 Improving Deep Neural Networks</li>
  <li>Course 3 Structured Machine Learning Projects</li>
  <li>Course 4 Convolutional Neural Networks</li>
  <li>Course 5 Sequence Models</li>
</ul>

<p>课程以订阅的形式提供，订阅的价格是$49/月，因此：<strong>学的越快，越省钱</strong>。极限情况下可以在7天试用期学完则一分钱不要。不过上班族很难吧，我计划1-2个月完成。</p>

<p>Course1，一共有4个week的课程，题目是：Neural Networks and Deep Learning</p>

<p>下面是Course1 Week1的笔记。</p>

<h1 id="welcome-to-deeplearning-specialization">Welcome to Deeplearning Specialization</h1>

<h2 id="welcome5min">welcome（5min）</h2>

<p>深度学习的光明前景：</p>

<blockquote>
  <p>AI is the new Electricity!</p>
</blockquote>

<p>5个Course的简介，以及Course1的目标：Recognize Cat</p>

<p>几个如雷贯耳的名词（先记下，后面慢慢学））：</p>
<ul>
  <li>CNN: Convolutional Neural Networks 卷积神经网络</li>
  <li>RNN: Recurrent Neural Networks 循环神经网络</li>
  <li>LSTM: Long Short-Term Memory 长短期记忆网络</li>
</ul>

<h1 id="introdcution-to-deep-learning">Introdcution to Deep Learning</h1>
<h2 id="whatis-a-neural-network-7min">what’is a neural network? (7min)</h2>

<p>Deep Learning = training (very large) neural network</p>

<p>What’s a neural network?</p>
<ul>
  <li>a very <strong>simple</strong> neural network: House Price-size Linear regression（就像一个Lego brick）
<img src="https://cdn.imshuai.com/images/2018/05/simple-neural-network.png" alt="simple-neural-network" /></li>
  <li>a <strong>larger</strong> neural network: stacking together many of these Lego bricks。
<img src="https://cdn.imshuai.com/images/2018/05/manual-larger-neural-network.png" alt="manual-larger-neural-network" /></li>
</ul>

<p>注意，上图只是一个手工演示的larger neural network，与实际的区别：</p>
<ol>
  <li>hidden layer并不是手工设置的</li>
  <li>每一层都是上一层所有输入的函数</li>
</ol>

<p>实际的neural network：
<img src="https://cdn.imshuai.com/images/2018/05/real-neural-network.png" alt="real-neural-network" /></p>

<p>Neural Network的两个特点；</p>
<ol>
  <li>Neural networks are remarkably good at <strong>figuring out functions that accurately map from x to y</strong>.</li>
  <li>Most powerful in supervised learning.</li>
</ol>

<p>一个常见的激活函数 ReLU ：
ReLU function: <strong>Re</strong>ctified <strong>Li</strong>near <strong>U</strong>nits (rectified means: taking a min of zero，原文是taking a <em>max</em> of zero，我觉得可能讲反了吧)
<img src="https://cdn.imshuai.com/images/2018/05/relu.png" alt="relu" /></p>

<h2 id="supervised-learning-with-neural-network-8min">Supervised Learning with Neural Network (8min)</h2>

<p>截止到目前，<strong>Neural Network的成功应用基本都在Supervised Learning</strong>。比如：Ad，Images vision, Audio to Text, Machine translation, Autonomous Driving<img src="https://cdn.imshuai.com/images/2018/05/supervised-learning-exmples.png" alt="supervised-learning-exmples" /></p>

<p>Neural Network examples:
<img src="https://cdn.imshuai.com/images/2018/07/NeuralNetworkExamples.jpg" alt="" /></p>

<p>Structured Data vs Unstructured Data</p>

<h2 id="why-is-deeplearning-taking-off-8min">Why is Deeplearning taking off? (8min)</h2>

<p><strong>Scale</strong> drives deep learning progress
<img src="https://cdn.imshuai.com/images/2018/05/Screen-Shot-2018-05-28-at-20.42.54.jpg" alt="Screen-Shot-2018-05-28-at-20.42.54" />
Scale means: Bigger network and More (labeled) data.</p>

<p>在数据匮乏的时代，算法的性能更依赖于技巧和手工设置的feature (the algorithms is actually not very well defined so if you don’t have a lot of training data is often up to your skill at hand engineering features )
大数据时代，大数据处于支配地位。(big data regime very large training sets very large M regime in the right that we more consistently see largely Ronettes dominating the other approaches)</p>

<p>三大原因：</p>
<ul>
  <li>Data</li>
  <li>Computation
比如：GPU</li>
  <li>Algorithm
比如：sigmoid function→ReLU function
<img src="https://cdn.imshuai.com/images/2018/05/Screen-Shot-2018-05-28-at-20.53.10.jpg" alt="Screen-Shot-2018-05-28-at-20.53.10" /></li>
</ul>

<p>算法必须快，形成一个正循环，快速验证：
<img src="https://cdn.imshuai.com/images/2018/05/Screen-Shot-2018-05-28-at-20.54.59.jpg" alt="Screen-Shot-2018-05-28-at-20.54.59" /></p>

<h1 id="hero-of-deep-learning">Hero of Deep Learning</h1>
<p>As part of this course by deeplearning.ai, hope to not just teach you the technical ideas in deep learning, but also <strong>introduce you to some of the people, some of the heroes in deep learning</strong>.（大概看了一下，后面还会介绍很多大神，这是比之前的Machine Learning增加的特色）</p>

<p>Geoffrey Hinton interview (40min)
<img src="https://cdn.imshuai.com/images/2018/05/Screen-Shot-2018-05-28-at-20.59.23.jpg" alt="Geoffrey Hinton interview" /></p>

<ul>
  <li>God father of deep learning</li>
  <li>传奇辗转的求学经历，为研究how does the brain store memories.：
    <ul>
      <li>Hologram made me interested in how does the <strong>brain store memories</strong>.</li>
      <li>Studying physiology and physics in Cambridege.</li>
      <li>Gave up and tried to do philosophy</li>
      <li>Switched to psychology.</li>
      <li>Took some time off and became a carpenter（瞎折腾这么久，也迷茫了😄）</li>
      <li>Then I decided that I’d try AI, and went of to Edinburgh</li>
      <li>Then get a PhD in AI.</li>
    </ul>
  </li>
  <li>前景惨淡
    <ul>
      <li>找不到工作</li>
      <li>in Britain, neural nets was regarded as kind of silly</li>
    </ul>
  </li>
  <li>看到曙光
    <ul>
      <li>in California, Don Norman and David Rumelhart were very open to ideas about neural nets. <strong>It was the first time I’d been somewhere where thinking about how the brain works</strong>（不忘初心啊）, and thinking about how that might relate to psychology, was seen as a very positive thing.</li>
      <li>1982与Rumelhart写了backpropagation论文, in UCSD。而实际上这又是一个重复发明了很多次的算法。但直到他们的工作，backpropagation才真正被人们重视。</li>
    </ul>
  </li>
  <li>最得意的算法发明：So I think the most beautiful one is the work I do with Terry Sejnowski on <strong>Boltzmann machines</strong>。
    <ul>
      <li><strong>Simple</strong> algorithm, beautiful.</li>
      <li>And it looked like the kind of thing you should be able to get in a brain because each synapse only needed to know about the behavior of the two neurons it was directly connected to.</li>
    </ul>
  </li>
  <li>ReLU</li>
  <li>Relationship between backpropagation and brains
    <ul>
      <li>I guess my main thought is this. If it turns out the back prop is a really good algorithm for doing learning. Then for sure evolution could’ve figured out how to prevent(字幕可能用错词了吧） it.</li>
      <li>And I think the brain probably has something that may not be exactly be backpropagation, but it’s quite close to it</li>
    </ul>
  </li>
  <li>Multiple time skills in deep learning
    <ul>
      <li>Fast weights</li>
    </ul>
  </li>
  <li>capsules
    <ul>
      <li>subsets as a capsule</li>
    </ul>
  </li>
  <li>your understanding of AI changed over these years?
    <ul>
      <li>most humain learning is unsupervised learning. What’s worked over the last ten years or so is supervised learning, <strong>in the long run, I think unsupervised learning is going to be absolutely crucial</strong>.  And things will work incredibly much better than they do now when we get that working properly, but we haven’t yet.</li>
      <li>I think generative adversarial nets are one of the sort of biggest ideas in deep learning that’s really new.</li>
    </ul>
  </li>
  <li>a sort of basic principle about how you model anything.
    <ul>
      <li>You take your measurements, and <strong>you’re applying nonlinear transformations to your measurements until you get to a representation as a state vector in which the action is linear</strong>. So you don’t just pretend it’s linear like you do with common filters. But you actually find a transformation from the observables to the underlying variables where linear operations, like matrix multipliers on the underlying variables, will do the work.</li>
    </ul>
  </li>
  <li>Advice for breaking through AI and deeplearning
    <ul>
      <li>My advice is sort of read the literature, but don’t read too much of it.Read a little bit of the literature. And notice something that you think everybody is doing wrong, I’m contrary in that sense. You look at it and it just doesn’t feel right. And then figure out how to do it right.</li>
      <li>Develop and trust your intuitions. When you have what you think is a good idea and other people think is complete rubbish, that’s the sign of a really good idea.</li>
      <li><strong>Never stop programming</strong>.
  <img src="https://cdn.imshuai.com/images/2018/05/never-stop-programming.jpg" alt="never-stop-programming" /></li>
    </ul>
  </li>
  <li>advice for new grad students
One good piece of advice for new grad students is, see if you can find an advisor who has beliefs similar to yours. Because if you work on stuff that your advisor feels deeply about, you’ll get a lot of good advice and time from your advisor. （这显然就是自己的亲身经历）</li>
  <li>I kind of agree with you, <strong>that it’s not quite a second industrial revolution, but it’s something on nearly that scale</strong>. And there’s a huge sea change going on, basically because <strong>our relationship to computers has changed. Instead of programming them, we now show them, and they figure it out</strong>.</li>
  <li>symbolic AI
And so I think thoughts are just these great big vectors, and that big vectors have causal powers. They cause other big vectors, and that’s utterly unlike the standard AI view that thoughts are symbolic expressions.</li>
</ul>
