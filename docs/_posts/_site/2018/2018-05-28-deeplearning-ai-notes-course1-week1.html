<p>==æç¤ºï¼š<strong>ä¸ºæ–¹ä¾¿é˜…è¯»ï¼Œã€Šdeeplearning.aiæ·±åº¦å­¦ä¹ ç¬”è®°ã€‹ç³»åˆ—åšæ–‡å·²ç»Ÿä¸€æ•´ç†åˆ° <a href="http://dl-notes.imshuai.com">http://dl-notes.imshuai.com</a></strong>==</p>

<p>Andrew Ngå»å¹´ç¦»å¼€ç™¾åº¦åï¼Œå†æ¬¡æŠ•èº«åˆ°äº†äººå·¥æ™ºèƒ½é¢†åŸŸçš„å¤§ä¼—æ•™è‚²ä¸Šï¼Œå¹¶æ¨å‡ºäº†<a href="http://www.deeplearning.ai">deeplearning.ai</a>ç½‘ç«™ï¼Œæ—¨åœ¨å»ºç«‹æ·±åº¦å­¦ä¹ æ•™è‚²å“ç‰Œã€‚ä¸è¿‡ç›®å‰è¯¾ç¨‹ä¸»è¦è¿˜æ˜¯æ‰˜ç®¡åœ¨Courseraä¸Šï¼Œdeeplearning.aiæœ¬èº«åªä¸è¿‡å°±æ˜¯ä¸€ä¸ª Landing pageã€‚</p>

<p>å­¦å®Œäº†æœºå™¨å­¦ä¹ ï¼ˆ<a href="/coursera-machine-learning-review/">Coursera Machine Learningæœºå™¨å­¦ä¹ è¯¾ç¨‹æ€»ç»“</a>ï¼‰ï¼Œæˆ‘è¦å¿«é©¬åŠ é­æŠŠæ·±åº¦å­¦ä¹ ä¹Ÿæä¸€éã€‚</p>

<p>ä¹‹å‰Machine Learningçš„ç¬”è®°éƒ½æ˜¯è®°åœ¨è‡ªå·±çš„Evernoteé‡Œé¢ï¼›æ—¢ç„¶è¦è®°å½•ï¼Œä¸ç„¶ç›´æ¥æ”¾åˆ°åšå®¢ï¼Œäº’ç›¸äº¤æµå²‚ä¸æ›´å¥½ã€‚</p>

<p>ç›®å‰åœ¨Courseraä¸Šä»¥Specializationçš„å½¢å¼å‡ºç°ï¼Œä¸€å…±åŒ…å«5ä¸ªCourseraï¼š</p>
<ul>
  <li>Course 1 Neural Networks and Deep Learning</li>
  <li>Course 2 Improving Deep Neural Networks</li>
  <li>Course 3 Structured Machine Learning Projects</li>
  <li>Course 4 Convolutional Neural Networks</li>
  <li>Course 5 Sequence Models</li>
</ul>

<p>è¯¾ç¨‹ä»¥è®¢é˜…çš„å½¢å¼æä¾›ï¼Œè®¢é˜…çš„ä»·æ ¼æ˜¯$49/æœˆï¼Œå› æ­¤ï¼š<strong>å­¦çš„è¶Šå¿«ï¼Œè¶Šçœé’±</strong>ã€‚æé™æƒ…å†µä¸‹å¯ä»¥åœ¨7å¤©è¯•ç”¨æœŸå­¦å®Œåˆ™ä¸€åˆ†é’±ä¸è¦ã€‚ä¸è¿‡ä¸Šç­æ—å¾ˆéš¾å§ï¼Œæˆ‘è®¡åˆ’1-2ä¸ªæœˆå®Œæˆã€‚</p>

<p>Course1ï¼Œä¸€å…±æœ‰4ä¸ªweekçš„è¯¾ç¨‹ï¼Œé¢˜ç›®æ˜¯ï¼šNeural Networks and Deep Learning</p>

<p>ä¸‹é¢æ˜¯Course1 Week1çš„ç¬”è®°ã€‚</p>

<h1 id="welcome-to-deeplearning-specialization">Welcome to Deeplearning Specialization</h1>

<h2 id="welcome5min">welcomeï¼ˆ5minï¼‰</h2>

<p>æ·±åº¦å­¦ä¹ çš„å…‰æ˜å‰æ™¯ï¼š</p>

<blockquote>
  <p>AI is the new Electricity!</p>
</blockquote>

<p>5ä¸ªCourseçš„ç®€ä»‹ï¼Œä»¥åŠCourse1çš„ç›®æ ‡ï¼šRecognize Cat</p>

<p>å‡ ä¸ªå¦‚é›·è´¯è€³çš„åè¯ï¼ˆå…ˆè®°ä¸‹ï¼Œåé¢æ…¢æ…¢å­¦ï¼‰ï¼‰ï¼š</p>
<ul>
  <li>CNN: Convolutional Neural Networks å·ç§¯ç¥ç»ç½‘ç»œ</li>
  <li>RNN: Recurrent Neural Networks å¾ªç¯ç¥ç»ç½‘ç»œ</li>
  <li>LSTM: Long Short-Term Memory é•¿çŸ­æœŸè®°å¿†ç½‘ç»œ</li>
</ul>

<h1 id="introdcution-to-deep-learning">Introdcution to Deep Learning</h1>
<h2 id="whatis-a-neural-network-7min">whatâ€™is a neural network? (7min)</h2>

<p>Deep Learning = training (very large) neural network</p>

<p>Whatâ€™s a neural network?</p>
<ul>
  <li>a very <strong>simple</strong> neural network: House Price-size Linear regressionï¼ˆå°±åƒä¸€ä¸ªLego brickï¼‰
<img src="https://cdn.imshuai.com/images/2018/05/simple-neural-network.png" alt="simple-neural-network" /></li>
  <li>a <strong>larger</strong> neural network: stacking together many of these Lego bricksã€‚
<img src="https://cdn.imshuai.com/images/2018/05/manual-larger-neural-network.png" alt="manual-larger-neural-network" /></li>
</ul>

<p>æ³¨æ„ï¼Œä¸Šå›¾åªæ˜¯ä¸€ä¸ªæ‰‹å·¥æ¼”ç¤ºçš„larger neural networkï¼Œä¸å®é™…çš„åŒºåˆ«ï¼š</p>
<ol>
  <li>hidden layerå¹¶ä¸æ˜¯æ‰‹å·¥è®¾ç½®çš„</li>
  <li>æ¯ä¸€å±‚éƒ½æ˜¯ä¸Šä¸€å±‚æ‰€æœ‰è¾“å…¥çš„å‡½æ•°</li>
</ol>

<p>å®é™…çš„neural networkï¼š
<img src="https://cdn.imshuai.com/images/2018/05/real-neural-network.png" alt="real-neural-network" /></p>

<p>Neural Networkçš„ä¸¤ä¸ªç‰¹ç‚¹ï¼›</p>
<ol>
  <li>Neural networks are remarkably good at <strong>figuring out functions that accurately map from x to y</strong>.</li>
  <li>Most powerful in supervised learning.</li>
</ol>

<p>ä¸€ä¸ªå¸¸è§çš„æ¿€æ´»å‡½æ•° ReLU ï¼š
ReLU function: <strong>Re</strong>ctified <strong>Li</strong>near <strong>U</strong>nits (rectified means: taking a min of zeroï¼ŒåŸæ–‡æ˜¯taking a <em>max</em> of zeroï¼Œæˆ‘è§‰å¾—å¯èƒ½è®²åäº†å§)
<img src="https://cdn.imshuai.com/images/2018/05/relu.png" alt="relu" /></p>

<h2 id="supervised-learning-with-neural-network-8min">Supervised Learning with Neural Network (8min)</h2>

<p>æˆªæ­¢åˆ°ç›®å‰ï¼Œ<strong>Neural Networkçš„æˆåŠŸåº”ç”¨åŸºæœ¬éƒ½åœ¨Supervised Learning</strong>ã€‚æ¯”å¦‚ï¼šAdï¼ŒImages vision, Audio to Text, Machine translation, Autonomous Driving<img src="https://cdn.imshuai.com/images/2018/05/supervised-learning-exmples.png" alt="supervised-learning-exmples" /></p>

<p>Neural Network examples:
<img src="https://cdn.imshuai.com/images/2018/07/NeuralNetworkExamples.jpg" alt="" /></p>

<p>Structured Data vs Unstructured Data</p>

<h2 id="why-is-deeplearning-taking-off-8min">Why is Deeplearning taking off? (8min)</h2>

<p><strong>Scale</strong> drives deep learning progress
<img src="https://cdn.imshuai.com/images/2018/05/Screen-Shot-2018-05-28-at-20.42.54.jpg" alt="Screen-Shot-2018-05-28-at-20.42.54" />
Scale means: Bigger network and More (labeled) data.</p>

<p>åœ¨æ•°æ®åŒ®ä¹çš„æ—¶ä»£ï¼Œç®—æ³•çš„æ€§èƒ½æ›´ä¾èµ–äºæŠ€å·§å’Œæ‰‹å·¥è®¾ç½®çš„feature (the algorithms is actually not very well defined so if you donâ€™t have a lot of training data is often up to your skill at hand engineering features )
å¤§æ•°æ®æ—¶ä»£ï¼Œå¤§æ•°æ®å¤„äºæ”¯é…åœ°ä½ã€‚(big data regime very large training sets very large M regime in the right that we more consistently see largely Ronettes dominating the other approaches)</p>

<p>ä¸‰å¤§åŸå› ï¼š</p>
<ul>
  <li>Data</li>
  <li>Computation
æ¯”å¦‚ï¼šGPU</li>
  <li>Algorithm
æ¯”å¦‚ï¼šsigmoid functionâ†’ReLU function
<img src="https://cdn.imshuai.com/images/2018/05/Screen-Shot-2018-05-28-at-20.53.10.jpg" alt="Screen-Shot-2018-05-28-at-20.53.10" /></li>
</ul>

<p>ç®—æ³•å¿…é¡»å¿«ï¼Œå½¢æˆä¸€ä¸ªæ­£å¾ªç¯ï¼Œå¿«é€ŸéªŒè¯ï¼š
<img src="https://cdn.imshuai.com/images/2018/05/Screen-Shot-2018-05-28-at-20.54.59.jpg" alt="Screen-Shot-2018-05-28-at-20.54.59" /></p>

<h1 id="hero-of-deep-learning">Hero of Deep Learning</h1>
<p>As part of this course by deeplearning.ai, hope to not just teach you the technical ideas in deep learning, but also <strong>introduce you to some of the people, some of the heroes in deep learning</strong>.ï¼ˆå¤§æ¦‚çœ‹äº†ä¸€ä¸‹ï¼Œåé¢è¿˜ä¼šä»‹ç»å¾ˆå¤šå¤§ç¥ï¼Œè¿™æ˜¯æ¯”ä¹‹å‰çš„Machine Learningå¢åŠ çš„ç‰¹è‰²ï¼‰</p>

<p>Geoffrey Hinton interview (40min)
<img src="https://cdn.imshuai.com/images/2018/05/Screen-Shot-2018-05-28-at-20.59.23.jpg" alt="Geoffrey Hinton interview" /></p>

<ul>
  <li>God father of deep learning</li>
  <li>ä¼ å¥‡è¾—è½¬çš„æ±‚å­¦ç»å†ï¼Œä¸ºç ”ç©¶how does the brain store memories.ï¼š
    <ul>
      <li>Hologram made me interested in how does the <strong>brain store memories</strong>.</li>
      <li>Studying physiology and physics in Cambridege.</li>
      <li>Gave up and tried to do philosophy</li>
      <li>Switched to psychology.</li>
      <li>Took some time off and became a carpenterï¼ˆçæŠ˜è…¾è¿™ä¹ˆä¹…ï¼Œä¹Ÿè¿·èŒ«äº†ğŸ˜„ï¼‰</li>
      <li>Then I decided that Iâ€™d try AI, and went of to Edinburgh</li>
      <li>Then get a PhD in AI.</li>
    </ul>
  </li>
  <li>å‰æ™¯æƒ¨æ·¡
    <ul>
      <li>æ‰¾ä¸åˆ°å·¥ä½œ</li>
      <li>in Britain, neural nets was regarded as kind of silly</li>
    </ul>
  </li>
  <li>çœ‹åˆ°æ›™å…‰
    <ul>
      <li>in California, Don Norman and David Rumelhart were very open to ideas about neural nets. <strong>It was the first time Iâ€™d been somewhere where thinking about how the brain works</strong>ï¼ˆä¸å¿˜åˆå¿ƒå•Šï¼‰, and thinking about how that might relate to psychology, was seen as a very positive thing.</li>
      <li>1982ä¸Rumelhartå†™äº†backpropagationè®ºæ–‡, in UCSDã€‚è€Œå®é™…ä¸Šè¿™åˆæ˜¯ä¸€ä¸ªé‡å¤å‘æ˜äº†å¾ˆå¤šæ¬¡çš„ç®—æ³•ã€‚ä½†ç›´åˆ°ä»–ä»¬çš„å·¥ä½œï¼Œbackpropagationæ‰çœŸæ­£è¢«äººä»¬é‡è§†ã€‚</li>
    </ul>
  </li>
  <li>æœ€å¾—æ„çš„ç®—æ³•å‘æ˜ï¼šSo I think the most beautiful one is the work I do with Terry Sejnowski on <strong>Boltzmann machines</strong>ã€‚
    <ul>
      <li><strong>Simple</strong> algorithm, beautiful.</li>
      <li>And it looked like the kind of thing you should be able to get in a brain because each synapse only needed to know about the behavior of the two neurons it was directly connected to.</li>
    </ul>
  </li>
  <li>ReLU</li>
  <li>Relationship between backpropagation and brains
    <ul>
      <li>I guess my main thought is this. If it turns out the back prop is a really good algorithm for doing learning. Then for sure evolution couldâ€™ve figured out how to prevent(å­—å¹•å¯èƒ½ç”¨é”™è¯äº†å§ï¼‰ it.</li>
      <li>And I think the brain probably has something that may not be exactly be backpropagation, but itâ€™s quite close to it</li>
    </ul>
  </li>
  <li>Multiple time skills in deep learning
    <ul>
      <li>Fast weights</li>
    </ul>
  </li>
  <li>capsules
    <ul>
      <li>subsets as a capsule</li>
    </ul>
  </li>
  <li>your understanding of AI changed over these years?
    <ul>
      <li>most humain learning is unsupervised learning. Whatâ€™s worked over the last ten years or so is supervised learning, <strong>in the long run, I think unsupervised learning is going to be absolutely crucial</strong>.  And things will work incredibly much better than they do now when we get that working properly, but we havenâ€™t yet.</li>
      <li>I think generative adversarial nets are one of the sort of biggest ideas in deep learning thatâ€™s really new.</li>
    </ul>
  </li>
  <li>a sort of basic principle about how you model anything.
    <ul>
      <li>You take your measurements, and <strong>youâ€™re applying nonlinear transformations to your measurements until you get to a representation as a state vector in which the action is linear</strong>. So you donâ€™t just pretend itâ€™s linear like you do with common filters. But you actually find a transformation from the observables to the underlying variables where linear operations, like matrix multipliers on the underlying variables, will do the work.</li>
    </ul>
  </li>
  <li>Advice for breaking through AI and deeplearning
    <ul>
      <li>My advice is sort of read the literature, but donâ€™t read too much of it.Read a little bit of the literature. And notice something that you think everybody is doing wrong, Iâ€™m contrary in that sense. You look at it and it just doesnâ€™t feel right. And then figure out how to do it right.</li>
      <li>Develop and trust your intuitions. When you have what you think is a good idea and other people think is complete rubbish, thatâ€™s the sign of a really good idea.</li>
      <li><strong>Never stop programming</strong>.
  <img src="https://cdn.imshuai.com/images/2018/05/never-stop-programming.jpg" alt="never-stop-programming" /></li>
    </ul>
  </li>
  <li>advice for new grad students
One good piece of advice for new grad students is, see if you can find an advisor who has beliefs similar to yours. Because if you work on stuff that your advisor feels deeply about, youâ€™ll get a lot of good advice and time from your advisor. ï¼ˆè¿™æ˜¾ç„¶å°±æ˜¯è‡ªå·±çš„äº²èº«ç»å†ï¼‰</li>
  <li>I kind of agree with you, <strong>that itâ€™s not quite a second industrial revolution, but itâ€™s something on nearly that scale</strong>. And thereâ€™s a huge sea change going on, basically because <strong>our relationship to computers has changed. Instead of programming them, we now show them, and they figure it out</strong>.</li>
  <li>symbolic AI
And so I think thoughts are just these great big vectors, and that big vectors have causal powers. They cause other big vectors, and thatâ€™s utterly unlike the standard AI view that thoughts are symbolic expressions.</li>
</ul>
