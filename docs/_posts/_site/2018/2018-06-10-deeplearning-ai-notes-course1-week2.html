<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>==æç¤ºï¼š<strong>ä¸ºæ–¹ä¾¿é˜…è¯»ï¼Œã€Šdeeplearning.aiæ·±åº¦å­¦ä¹ ç¬”è®°ã€‹ç³»åˆ—åšæ–‡å·²ç»Ÿä¸€æ•´ç†åˆ° <a href="http://dl-notes.imshuai.com">http://dl-notes.imshuai.com</a></strong>==</p>

<p>Basics of Neural Network Programming</p>
<h1 id="logistic-regression-as-a-neural-network">Logistic Regression as a Neural Network</h1>

<h2 id="binary-classification">Binary Classification</h2>
<ol>
  <li>Example: ç»™ä¸€å¼ 64x64åƒç´ çš„å›¾ç‰‡å›¾ç‰‡ï¼Œåˆ¤æ–­æ˜¯å¦å«æœ‰çŒ«</li>
  <li>è·å–å›¾ç‰‡çš„RGBåƒç´ å€¼
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-02-at-17.20.31.jpg" alt="Screen-Shot-2018-06-02-at-17.20.31" /></li>
  <li>å¹¶unrollæˆä¸€ä¸ªvector \(X^{(i)}\)
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-02-at-17.20.42.jpg" alt="Screen-Shot-2018-06-02-at-17.20.42" /></li>
  <li>
    <p>æ‰€æœ‰çš„vectorç»„æˆæ•°æ®é›†çŸ©é˜µ\(X\)
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-02-at-17.25.21.jpg" alt="Screen-Shot-2018-06-02-at-17.25.21" />
ç‰¹åˆ«æ³¨æ„ï¼Œ\(X\)çš„è¡Œæ˜¯\(n\)ï¼Œåˆ—æ˜¯\(m\)ï¼Œå’ŒMachine learningä¸­çš„å®šä¹‰æ­£å¥½æ˜¯è½¬ç½®çš„å…³ç³»ã€‚è¿™æ ·æœ‰ä¸ªå¥½å¤„ï¼Œæ¯æ¡æµ‹è¯•é›†åœ¨çŸ©é˜µä¸­éƒ½æ˜¯ä»¥åˆ—å‘é‡çš„å½¢å¼å­˜åœ¨ã€‚</p>
  </li>
  <li>DeepLearningå¸¸ç”¨Notations:
    <ul>
      <li>\(m\): number of examples in datasets</li>
      <li>\(n_x\): input sizeï¼ˆå³featureçš„ä¸ªæ•°ï¼‰</li>
      <li>\(n_y\): output sizeï¼ˆå³åˆ†ç±»ä¸ªæ•°ï¼‰</li>
      <li>\(X \in \mathbb{R}^{n_x\times m} \) ï¼šthe input matrix</li>
      <li>\(Y \in \mathbb{R}^{n_y\times m} \) ï¼šthe input matrix</li>
      <li>å¸¦æ‹¬å·çš„ä¸Šæ ‡\(^{(i)}\)ï¼Œè¡¨ç¤ºå’Œtraining exampleç›¸å…³çš„è®¡æ•°</li>
    </ul>
  </li>
</ol>

<p>å®Œæ•´çš„notationï¼Œå¯ä»¥å‚è€ƒè¯¾ç¨‹ä¸­æä¾›çš„PDF: <em>Standard notations for Deep Learning</em>
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-10-at-21.21.45.jpg" alt="Screen-Shot-2018-06-10-at-21.21.45" />
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-10-at-21.21.58.jpg" alt="Screen-Shot-2018-06-10-at-21.21.58" /></p>

<ol>
  <li>ä½¿ç”¨Pythonä¸­çš„reshapeæ–¹æ³•ï¼Œæ•´ç†çŸ©é˜µçš„ç»´åº¦ã€‚</li>
</ol>

<h2 id="logistic-regression">Logistic Regression</h2>
<ol>
  <li>é—®é¢˜æè¿°ï¼š
Logsitic Regressionè¦æ±‚è¾“å‡ºyä¸æ˜¯0å°±æ˜¯1ã€‚The goal of logistic regression is to minimize the error between its predictions and training data.
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-02-at-17.37.35.jpg" alt="Screen-Shot-2018-06-02-at-17.37.35" /></li>
  <li>sigmoid function
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-02-at-17.45.47.jpg" alt="Screen-Shot-2018-06-02-at-17.45.47" />
==è¿™é‡Œæœ‰ä¸ªç–‘é—®ï¼Œä¸ºä»€ä¹ˆsigmoidå¤„ç†åçš„å€¼ï¼Œå¯ä»¥ä»£è¡¨y=1æ¦‚ç‡ï¼Ÿ==
å‚è€ƒï¼š<a href="https://stats.stackexchange.com/questions/80611/problem-understanding-the-logistic-regression-link-function/80623#80623">Logistic distribution</a></li>
  <li>å¼•å…¥å‚æ•°w, bï¼Œå…¶å®å°±æ˜¯Machine learningä¸­ç”¨çš„æ˜¯Î¸ï¼Œä½†DeepLearningä¸­åˆ†åˆ«ç”¨wå’Œbè¡¨ç¤ºã€‚å…¶ä¸­wæ˜¯vectorï¼Œbæ˜¯real number</li>
  <li>è¿™é‡Œ ğ‘¦Ì‚ å°±æ˜¯Machine learningé‡Œé¢çš„hypothesis function: h(Î¸)</li>
</ol>

<h2 id="logistic-regression-cost-funciton">Logistic Regression Cost Funciton</h2>

<ol>
  <li>
    <p>Loss (error) functionçš„å®šä¹‰ï¼š
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-02-at-17.52.42.jpg" alt="Screen-Shot-2018-06-02-at-17.52.42" />
åˆ†æˆyä¸º0å’Œ1ä¸¤ç§æƒ…å†µå»ç†è§£è¿™ä¸ªå‡½æ•°ï¼Œ<strong>æœ¬è´¨ä¸Šå°±æ˜¯å¯¹ğ‘¦Ì‚åšå¯¹æ•°å¤„ç†</strong>è€Œå·²ã€‚
å› ä¸ºå¯¹æ•°å¤„ç†åç¡®å®<strong>è¾¾åˆ°äº†Loss functionçš„è¦æ±‚</strong>ï¼ˆæˆ‘è‡ªå·±çš„ç†è§£ï¼‰ï¼š1.å€¼åŸŸæ˜¯å¤§äºç­‰äº0çš„å®æ•°é›†ã€‚ 2. éšç€ğ‘¦Ì‚ å•è°ƒé€’å‡ã€‚y=ğ‘¦Ì‚ çš„æ—¶å€™ä¸º0ï¼Œåä¹‹è¶‹å‘äºâˆã€‚3. æ˜¯å‚æ•°çš„å‡¸å‡½æ•°ï¼ˆconvexï¼‰4. æ˜¯yå’Œğ‘¦Ì‚çš„å‡½æ•°</p>
  </li>
  <li>æ²¡æœ‰ä½¿ç”¨square errorï¼Œå› ä¸ºæ˜¯non-convexï¼Œæ— æ³•ä½¿ç”¨Gradient Descentç®—æ³•</li>
  <li>Loss functionæ˜¯é’ˆå¯¹å•ä¸ªtraining exampleçš„ï¼Œè€Œ<strong>Cost functionæ˜¯Loss Functionçš„åœ¨æ‰€æœ‰training exampleä¸Šçš„å‡å€¼</strong>ã€‚
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-02-at-18.03.20.jpg" alt="Screen-Shot-2018-06-02-at-18.03.20" />
åœ¨Machine learningé‡Œï¼Œæ²¡æœ‰å¼•å…¥Loss Functionï¼Œå…¶å®æœ‰ä¸€ä¸ªLoss Functionï¼Œæ›´å¥½ç†è§£ã€‚</li>
</ol>

<h2 id="gradient-descent">Gradient Descent</h2>
<p>Gradient Descentçš„åŸç†ï¼ˆIntuitionï¼‰ï¼šæŒ‰æ¢¯åº¦æœ€å¤§çš„æ–¹å‘é€¼è¿‘æœ€å°å€¼ã€‚
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-02-at-20.48.07.jpg" alt="Screen-Shot-2018-06-02-at-20.48.07" /></p>

<p>Gradient Descentç®—æ³•æ­¥éª¤ï¼š</p>
<ol>
  <li>Initialize \(w\), \(b\) to zero</li>
  <li>repeatï¼š</li>
</ol>

<p><script type="math/tex">w\ :=w - w\frac{\partial J( w,b)}{\partial w}</script>
<script type="math/tex">b\ :=b - b\frac{\partial J( w,b)}{\partial b}</script></p>

<h2 id="derivatives">Derivatives</h2>
<p>ä¸ºä¸äº†è§£å¯¼æ•°çš„äººä»‹ç»å¯¼æ•°çš„ç›´è§‚å«ä¹‰ï¼Œè¿™é‡Œä¸ä½œè¯´æ˜äº†ã€‚</p>

<h2 id="more-derivative-examples">More Derivative Examples</h2>
<p>ä¸ºä¸äº†è§£å¯¼æ•°çš„äººä»‹ç»å¯¼æ•°çš„ç›´è§‚å«ä¹‰ï¼Œè¿™é‡Œä¸ä½œè¯´æ˜äº†ã€‚</p>

<h2 id="computation-graph">Computation graph</h2>
<p>ä»å·¦åˆ°å³ï¼šè®¡ç®—å‡½æ•°J
ä»å³åˆ°å·¦ï¼šè®¡ç®—Jå¯¹å‚æ•°wå’Œbå¯¼æ•°</p>

<h2 id="derivative-with-a-computation-graph">Derivative with a Computation Graph</h2>
<ul>
  <li>å…¶å®æ˜¯å°±æ˜¯å¤åˆå‡½æ•°çš„é“¾å¼æ³•åˆ™ã€‚</li>
  <li>è®¡ç®—å›¾å·¦è¾¹çš„å˜é‡çš„åå¯¼æ•°ä¾èµ–äºå³è¾¹çš„åå¯¼æ•°ï¼Œå³è¾¹çš„åå¯¼æ•°è®¡ç®—åï¼Œå¯ä»¥è¢«å·¦è¾¹çš„è®¡ç®—å¤ç”¨ã€‚</li>
</ul>

<p>åœ¨Pythonä¸­è¡¨ç¤ºåå¯¼æ•°
<script type="math/tex">dvar = \frac{\partial J}{\partial var}</script></p>

<h2 id="logistic-regression-gradient-descent">Logistic Regression Gradient Descent</h2>
<p>ä½¿ç”¨Computation Graphè®¡ç®—</p>

<p><img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-04-at-08.26.07.jpg" alt="Screen-Shot-2018-06-04-at-08.26.07" /></p>

<p>è™½ç„¶ï¼Œæµ‹è¯•é›†æ˜¯ç¦»æ•£çš„ï¼Œä½†å¹¶ä¸ä»£è¡¨å¯¹wçš„å€’æ•°æ˜¯ç¦»æ•£çš„ï¼Œè¿™ä¸¤è€…æ²¡æœ‰ä»»ä½•å…³ç³»ã€‚<strong>å§‹ç»ˆæ³¨æ„ï¼šåœ¨gradient Descentçš„æ—¶å€™ï¼Œxæ˜¯å¸¸é‡</strong></p>

<h2 id="gradient-descent-on-m-examples">Gradient Descent on m Examples</h2>
<p>Cost Functionçš„åå¯¼æ˜¯Loss Functionåå¯¼çš„å‡å€¼ï¼š
<script type="math/tex">\frac{\partial J(w,b)}{\partial w_j} =\frac{1}{m} \sum_{i=1}^{m} \frac{\partial \mathcal{L}(a^{(i)},y^{(i)})}{\partial w_j^{(i)}}</script></p>

<p>Gradient Descentç®—æ³•è¿‡ç¨‹ï¼š</p>
<ol>
  <li>æ±‚å¯¼è¿‡ç¨‹
 æ±‚å¯¼è¿‡ç¨‹åˆé€šå¸¸æ˜¯å…ˆforward propagationæ±‚cost functionï¼Œç„¶åå†backward propagationæ±‚åˆ°wå’Œbçš„å€’æ•°</li>
  <li>ä¸‹é™è¿‡ç¨‹
ä½¿ç”¨åˆ°wå’Œbçš„å¯¼æ•°ï¼Œè¿­ä»£åšæ¢¯åº¦ä¸‹é™è¿‡ç¨‹ã€‚</li>
</ol>

<p>ä¸‹é¢çš„æˆªå›¾å°±æ˜¯ä¸€ä¸ªéå‘é‡åŒ–çš„å®ç°ï¼š
å·¦è¾¹æ˜¯æ±‚å¯¼è¿‡ç¨‹ï¼Œå³è¾¹æ˜¯æ¢¯åº¦ä¸‹é™è¿‡ç¨‹
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-04-at-09.02.27.jpg" alt="Screen-Shot-2018-06-04-at-09.02.27" /></p>

<h1 id="python-and-vectorization">Python and Vectorization</h1>
<h2 id="vectorization">Vectorization</h2>
<ol>
  <li>
    <p>ä»€ä¹ˆæ˜¯Vectorizationï¼šå°† for loop å°½å¯èƒ½è½¬æ¢ä¸ºçŸ©é˜µè¿ç®—ã€‚ä¸¾ä¾‹ï¼š
<script type="math/tex">z = w^Tx + b</script>
<img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-04-at-09.13.25.jpg" alt="Screen-Shot-2018-06-04-at-09.13.25" /></p>
  </li>
  <li>
    <p>vectorizationçš„å¥½å¤„ï¼šconciser code, but faster execution
ä¸€ä¸ªç®€å•çš„å¯¹æ¯”å®éªŒï¼š1,000,000å¤§å°çš„ä¸¤ä¸ªå‘é‡å†…ç§¯è®¡ç®—ï¼Œfor loopè¦æ¯”Vectorizationå¿«300å€ã€‚
åœ¨DeepLearningæ—¶ä»£ï¼Œvectorizationæ˜¯ä¸€é¡¹é‡è¦çš„æŠ€èƒ½ã€‚</p>
  </li>
  <li>
    <p>SIMD
Both CPU and GPU have parallelization instructions(i.e. SIMD, Signle Instruction Multiple Data)</p>
  </li>
</ol>

<h2 id="more-vectorization-examples">More vectorization Examples</h2>
<ol>
  <li>åŸåˆ™ï¼šwhenever possible, avoid explict for-loops</li>
  <li>ä½¿ç”¨Element wisedçš„çŸ©é˜µè¿ç®—ï¼Œå°†å‡½æ•°ä½œç”¨åœ¨æ¯ä¸ªçŸ©é˜µå…ƒç´ ä¸Šï¼Œæ¯”å¦‚ï¼š
    <ul>
      <li>np.exp()</li>
      <li>np.log()</li>
      <li>np.abs()</li>
      <li>np.maxium()</li>
      <li>1/v</li>
      <li>v**2</li>
    </ul>
  </li>
</ol>

<h2 id="vectorizing-logistic-regression">Vectorizing logistic Regression</h2>

<p><img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-04-at-19.50.07.jpg" alt="Screen-Shot-2018-06-04-at-19.50.07" /></p>

<script type="math/tex; mode=display">A = \sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})</script>

<p>ï¼ˆæˆ‘å§‹ç»ˆè§‰å¾—Aåº”è¯¥å°å†™ï¼Œæ¯•ç«Ÿè¿˜æ˜¯ä¸€ä¸ªè¡Œå‘é‡ï¼‰</p>

<p>ä¸ªäººç»éªŒï¼š</p>
<ol>
  <li>é¦–å…ˆï¼Œç†Ÿæ‚‰æ¯ä¸ªå˜é‡çš„è®°å·å’Œç»´åº¦ï¼Œå¿…è¦çš„è¯ï¼Œå¯ä»¥ç”»å‡ºæ¥ï¼Œæ›´ç›´è§‚ã€‚</li>
  <li>å…ˆä»ä¸€ä¸ªæ ·æœ¬åšå‘é‡åŒ–ï¼Œå†æŠŠmä¸ªæ ·æœ¬çš„æ“ä½œå‘é‡åŒ–ã€‚</li>
  <li>for-loopé‡Œé¢æ˜¯å¾ªç¯ä¹˜æ³•ï¼Œåˆ™å‘é‡åŒ–ä¸€å®šæ˜¯ä¸€ä¸ªä¹˜æ³•å½¢å¼ï¼Œè‹¥å¯¹äºä¸ç¡®å®šä¹˜æ³•çš„å·¦å³å…³ç³»ï¼Œæ˜¯å¦éœ€è½¬ç½®ï¼Œå¯ä»¥æ ¹æ®ç›®æ ‡å˜é‡çš„ç»´åº¦æ¨æµ‹ã€‚æˆ–è€…å…ˆä¹˜èµ·æ¥ï¼Œå†æ ¹æ®ç›®æ ‡å˜é‡çœ‹æ˜¯å¦è¦è½¬ç½®ã€‚</li>
</ol>

<h2 id="vectorizing-logistic-regressions-gradient-output">Vectorizing Logistic Regressionâ€™s Gradient Output</h2>

<p>æ¨å¯¼è¿‡ç¨‹</p>

<p><img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-09-at-18.35.35.jpg" alt="Screen-Shot-2018-06-09-at-18.35.35" /></p>

<p>æœ€ç»ˆå‘é‡åŒ–çš„å½¢å¼æ˜¯ï¼š</p>

<p><script type="math/tex">\frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T</script>
<script type="math/tex">\frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})</script></p>

<h2 id="broadcasting-in-python">Broadcasting in Python</h2>
<p>åœ¨matlabå’ŒPythonä¸­ï¼Œéƒ½é»˜è®¤æ”¯æŒä¸åŒç»´åº¦çš„å˜é‡åšelement-wisedçš„è®¡ç®—ï¼ˆ+-x/ç­‰ï¼‰ã€‚æ‰€è°“<strong>Broadcasting</strong>å…¶å®å°±æ˜¯é«˜çº¬åº¦æ•°ç»„å’Œä½çº¬åº¦æ•°ç»„è®¡ç®—æ—¶ï¼Œå°†ä½ç»´çš„å˜é‡<strong>é€šè¿‡å¤åˆ¶çš„æ–¹å¼å‘é«˜ç»´æ‰©å±•ç»´åº¦</strong>ï¼Œå†åšè¿ç®—ã€‚ä½†è¦æ³¨æ„ï¼š</p>
<ol>
  <li>ä½çº¬åº¦æ•°ç»„ä¸é«˜çº¬åº¦å…±æœ‰çš„ç»´åº¦ï¼Œå…ƒç´ ä¸ªæ•°å¿…é¡»ä¸€æ ·ï¼Œæ¯”å¦‚ä¸€ä¸ªshapeæ˜¯(5,3)çš„æ•°ç»„å¯ä»¥å’Œä¸€ä¸ªshapeæ˜¯(5,)çš„æ•°ç»„ç›¸åŠ ï¼Œä½†ä¸èƒ½å’Œä¸€ä¸ª(4,)çš„æ•°ç»„ç›¸åŠ ã€‚</li>
  <li>ç»´åº¦ç›¸åŒçš„æ•°ç»„çš„æ•°ç»„ï¼Œæ˜¯ä¸èƒ½broadcastingçš„ï¼Œæ¯”å¦‚ä¸€ä¸ªshapeæ˜¯(5,3)çš„æ•°ç»„å’Œä¸€ä¸ªshapeæ˜¯(5,2)çš„æ•°ç»„è¿ç®—ï¼Œåè€…æ˜¯æ— æ³•broadcastingçš„</li>
  <li>æŸä¸ªç»´åº¦çš„ä¸ªæ•°æ˜¯1ï¼Œç­‰åŒäºè¿™ä¸ªç»´åº¦ä¸å­˜åœ¨ï¼Œå¯ä»¥broadcastingï¼Œæ¯”å¦‚shapeæ˜¯(5,3)å’Œ(5,1)çš„æ•°ç»„å¯ä»¥è¿ç®—ã€‚</li>
</ol>

<p>æ€»çš„æ¥è¯´ï¼Œå°±æ˜¯broadcastingè¦åšæŸä¸ªç»´åº¦çš„å¤åˆ¶ï¼Œå¿…é¡»åœ¨èµ‹å€¼çš„æ—¶å€™è¡Œå¾—é€šã€‚æ¯”å¦‚shapeæ˜¯(5,3)å’Œshapeæ˜¯(5,2)çš„æ•°ç»„è¿ç®—ï¼Œåè€…è¦å°†2å¤åˆ¶ä¸º3æ˜¯è¡Œä¸é€šçš„ï¼Œå› ä¸ºå­˜åœ¨ä¸¤è¡Œï¼Œé‚£å–å“ªä¸€è¡Œï¼Ÿ</p>

<p>Andrewçš„ä¸€ä¸ªç»éªŒï¼šå¦‚æœå¯¹æŸä¸ªæ•°ç»„çš„shapeä¸ç¡®å®šï¼Œå¯ä»¥ç”¨reshapeæ˜¾å¼çš„è°ƒç”¨ä¸€ä¸‹ï¼Œç¡®ä¿ç»´åº¦æ­£ç¡®ã€‚</p>

<p>è¡¥å……ï¼šnumpyä¸­ï¼Œç±»ä¼¼sumçš„å‡½æ•°ï¼Œç»å¸¸æ¶‰åŠaxiså‚æ•°ï¼Œå¯ä»¥å–å€¼ä¸º0æˆ–1ï¼Œç”šè‡³å…¶ä»–ã€‚ç»å¸¸è®°ä¸ä½ï¼Œè¿™é‡Œæˆ‘æŸ¥äº†äº†ä¸€ä¸‹ï¼Œæ˜¯è¿™æ ·çš„ï¼ˆ<a href="https://stackoverflow.com/questions/17079279/how-is-axis-indexed-in-numpys-array">åŸæ–‡</a>ï¼‰ï¼š</p>
<ol>
  <li><strong>axisçš„æ•°å­—ï¼Œå’Œæ•°ç»„çš„shapeå‚æ•°çš„ç´¢å¼•æ˜¯å¯¹åº”çš„</strong>ã€‚æ¯”å¦‚ä¸€ä¸ªæ•°ç»„çš„shapeæ˜¯(5,6)ï¼Œåˆ™ä»£è¡¨5ä¸ªrowï¼Œ6ä¸ªcolumnã€‚å³åœ¨shapeä¸­ï¼Œrowå’Œcolumnçš„ä¸ªæ•°çš„ç´¢å¼•æ˜¯0å’Œ1ã€‚ä¹Ÿå°±ç¬¬1ä¸ªåæ ‡ï¼Œåœ¨shapeä¸­çš„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œ<strong>ç´¢å¼•æ˜¯0ï¼Œä»£è¡¨rowçš„æ–¹å‘</strong>ï¼›ç¬¬2ä¸ªåæ ‡ï¼Œåœ¨shapeä¸­çš„ç¬¬2ä¸ªå…ƒç´ ï¼Œ<strong>ç´¢å¼•æ˜¯1ï¼Œä»£è¡¨rowçš„æ–¹å‘</strong>ã€‚</li>
  <li>å¯¹äºsumå‡½æ•°ï¼ŒaxisæŒ‡çš„æ˜¯sumâ€œ<strong>æ²¿ç€</strong>â€çš„æ–¹å‘ï¼Œç»è¿‡è®¡ç®—ï¼Œè¿™ä¸ªæ–¹å‘çš„ç»´åº¦å› ä¸ºæ±‚å’Œåå°±æ¶ˆå¤±äº†ï¼Œæ¯”å¦‚sum(axis=0)ä»£è¡¨æ˜¯æ²¿ç€â€œrowâ€æ–¹å‘è¿›è¡Œæ±‚å’Œï¼Œ</li>
  <li>å½“ç„¶axiså¯ä»¥æ˜¯ä¸€ä¸ªtupeï¼Œé‚£å°±ç›¸å½“äºæ²¿ç€å¤šä¸ªå¤šä¸ªæ–¹å‘æ±‚å’Œã€‚</li>
  <li>sumå¦‚æœä¸ä¼ å…¥axiså‚æ•°ï¼Œé»˜è®¤æ˜¯å¯¹æ‰€æœ‰ç»´åº¦æ±‚å’Œã€‚</li>
</ol>

<h2 id="a-note-on-pythonnumpy-vectors">A note on python/numpy vectors</h2>
<p>broadcastingçš„ä¸€ä¸ªå¼±ç‚¹ï¼šå¯èƒ½éšè—æ½œåœ¨çš„é”™è¯¯ï¼Œæ¯”å¦‚ä¸€ä¸ªè®¡ç®—ä¸­æœ¬æ¥è¦å»ä¸¤ä¸ªè¿ç®—çš„æ•°ç»„ç»´åº¦ä¸€æ ·ï¼Œå¦‚æœæ²¡æœ‰broadcastingï¼Œå°±ä¼šç›´æ¥æŠ¥é”™ï¼›è€Œbroadcastingå…è®¸å¯ç»§ç»­æ‰§è¡Œã€‚</p>

<p><strong>rank 1 arrayé—®é¢˜</strong>ï¼šshapeæ˜¯(x,)çš„æ•°ç»„ï¼Œæ—¢ä¸æ˜¯è¡Œå‘é‡ï¼Œä¹Ÿä¸æ˜¯åˆ—å‘é‡ï¼Œæ²¡æ³•å‚ä¸æ­£å¸¸çš„çŸ©é˜µè¿ç®—ï¼Œåº”è¯¥æ€»æ˜¯ä½¿ç”¨(x,1)æˆ–(1,x)çš„shapeæ¥è¡¨ç¤ºå‘é‡ã€‚ä½†å¯ä»¥é€šè¿‡reshapeæ–¹æ³•å°†rank 1 arrayè½¬æ¢ä¸ºè¡Œå‘é‡æˆ–åˆ—å‘é‡ã€‚ï¼ˆä»€ä¹ˆæ˜¯rankï¼Œå°±æ˜¯ä¸€ä¸ªæ•°ç»„çš„ç»´åº¦ï¼‰</p>

<p><img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-10-at-18.03.37.jpg" alt="Screen-Shot-2018-06-10-at-18.03.37" /></p>

<h2 id="quick-tour-of-jupyteripython-notebooks">Quick tour of Jupyter/iPython Notebooks</h2>

<p>è¿™éƒ¨åˆ†ï¼Œä¹Ÿå¯ä»¥å‚è€ƒæˆ‘ä¹‹å‰çš„æ–‡ç« ï¼šã€Š<a href="/jupyter-notebook-introduction-configuration/">Jupyter Notebookç®€ä»‹å’Œé…ç½®è¯´æ˜</a>ã€‹</p>

<h2 id="explanation-of-logistic-regression-cost-function-optional">Explanation of logistic regression cost function (optional)</h2>
<p>è®¡ç®—ç»“æœğ‘¦Ì‚ä»£è¡¨äº†ç»™å®šæ ·æœ¬xï¼Œy=1çš„æ¦‚ç‡ï¼Œå³ğ‘¦Ì‚=P(y=1|x)</p>

<p>ä½†ä¸ºä»€ä¹ˆå¯ä»¥å¯¹åº”åˆ°æ¦‚ç‡å‘¢ï¼Ÿå‚è€ƒï¼šä¸ºä»€ä¹ˆsigmoidå¯ä»¥ä»£è¡¨æ¦‚ç‡ï¼Œæ¶‰åŠLogistic distribution
https://stats.stackexchange.com/questions/80611/problem-understanding-the-logistic-regression-link-function/80623#80623</p>

<table>
  <tbody>
    <tr>
      <td>Loss functionå…¶å®å°±æ˜¯å¯¹æ¦‚ç‡P(y</td>
      <td>x)å–å¯¹æ•°ï¼š</td>
    </tr>
  </tbody>
</table>

<p><img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-10-at-18.28.41.jpg" alt="Screen-Shot-2018-06-10-at-18.28.41" /></p>

<p>Loss functionè¶Šå°ï¼Œåˆ™å–åˆ°å’Œå®é™…å€¼yçš„æ¦‚ç‡è¶Šå¤§ã€‚</p>

<p>æ‰€æœ‰æ ·æœ¬çš„Cost functionï¼š</p>

<p><img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-10-at-18.32.40.jpg" alt="Screen-Shot-2018-06-10-at-18.32.40" /></p>

<h1 id="heros-of-deep-learningpieter-abbeel-interview">Heros of Deep Learningï¼šPieter Abbeel interview</h1>

<p><img src="https://cdn.imshuai.com/images/2018/06/Screen-Shot-2018-06-10-at-18.34.23.jpg" alt="Screen-Shot-2018-06-10-at-18.34.23" /></p>

<ol>
  <li>
    <p>Pieter Abbeelä¸“æ³¨äºdeep reinforcement learning.</p>
  </li>
  <li>advice for people entrying AI
    <ul>
      <li>A good time to enter AI. High demand.</li>
      <li>Not just read things or watch videos but <strong>try things out</strong>.</li>
      <li>With frameworks like <strong>TensorFlow, Chainer, Theano, PyTorch and so forth</strong>, itâ€™s very easy to get going and get something up and running very quickly.</li>
    </ul>
  </li>
  <li>
    <p>Andrew Ng: We live in good times. If people want to learn.</p>
  </li>
  <li>what are the things that deep reinforcement learning is already working really well at?
    <ul>
      <li>learning to play games from pixels</li>
      <li>robot inventing walking, running, standing up with a single algorithm.</li>
    </ul>
  </li>
</ol>

