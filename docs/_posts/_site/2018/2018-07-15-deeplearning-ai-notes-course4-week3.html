<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>本周介绍了目标检测算法，包括使用bounding box定位，sliding window算法以及YOLO算法。其中YOLO算法涉及IoU、Non-max Suppression、Anchor Boxes。</p>

<h1 id="1--detection-algorithm">1- Detection Algorithm</h1>
<p>目标检测（Object detection），在近几年发展的很迅猛。<strong>目标检测的第一步是目标定位（Object Localization）</strong></p>

<p>图形识别相关的问题可以分为下面三个层次：</p>
<ul>
  <li>Image Classification：判断图片中是否包含某物。</li>
  <li>Classification with localization：判断图片是否包含某物，以及某物的边框。</li>
  <li>Detection：图片中包含多个属于不同类别的对象，识别出包含的所有对象，以及其边框。</li>
</ul>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-11_21-39-23-1.jpg" alt="Xnip2018-07-11_21-39-23" /></p>

<p>其中前两个问题通常在图片中央有一个目标对象。</p>

<p>本周学习的是第二个问题。</p>

<h2 id="11--object-localization">1.1- Object Localization</h2>

<p>根据定位目标的个数，可以分为单物体定位（1 Object Localization）和多物体定位（Multiple Objects Localization）。本周先研究前者，即Classification with localization。</p>

<ol>
  <li>bounding box</li>
</ol>

<p>目标的具体位置，可以用<strong>边框（bounding box）</strong> 来表示， <strong>bounding box即一个定位物体的矩形框</strong>，如下图：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-10_21-19-48.jpg" alt="Xnip2018-07-10_21-19-48" /></p>

<p>框住汽车的bounding box可以用四个数字表示：</p>
<ul>
  <li>\(b_x\): 物体中心点的x坐标</li>
  <li>\(b_y\): 物体中心点的y坐标</li>
  <li>\(b_h\): 矩形框的高度</li>
  <li>\(b_w\): 矩形框的宽度</li>
</ul>

<p>其中坐标系一般选择左上角为原点，右下角为(1,1)，如下图：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-10_21-30-07.jpg" alt="Xnip2018-07-10_21-30-07" /></p>

<p>上图可能的值是\(b_x=0.5\)， \(b_y=0.7\)， \(b_w=0.4\)， \(b_h=0.3\)。</p>

<ol>
  <li>Classification with localization</li>
</ol>

<p>Classification问题，最终通过softmax输出one-hot向量，表示属于哪个类型的对象，比如一个路面识别四种类型（1-行人 2-车辆 3-摩托车 4-背景）的Classification的卷积网络如下：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-11_21-48-30.jpg" alt="Xnip2018-07-11_21-48-30" /></p>

<p>Classification with localization则在上述基础上，增加bounding box坐标的输出：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-11_21-50-17.jpg" alt="Xnip2018-07-11_21-50-17" /></p>

<ol>
  <li>Defining the target label y</li>
</ol>

<p>在classification问题，只用定义一个one-hot向量，表示某个物体是否存在，而物体定位，需要在此基础上扩充为：</p>
<ul>
  <li>\(p_c\)： 表示物体是否存在</li>
  <li>bounding box的四个参数：\(b_x, b_y, b_h, b_w\)</li>
  <li>物体分类的one-hot向量</li>
</ul>

<p>比如上面的路面识别的例子，对应的标记向量y可以表示为形式：
<script type="math/tex">\\left[
 \\begin{matrix}
   p_c \\\\
   b_x \\\\
   b_y \\\\
   b_h \\\\
   b_w \\\\
   c_1 \\\\
   c_2 \\\\
   c_3 \\\\
   c_4 \\\\
  \end{matrix}
  \\right]</script></p>

<p>存在一个车辆的向量y就是：</p>

<script type="math/tex; mode=display">\\left[
 \\begin{matrix}
   1 \\\\
   0.5 \\\\
   0.7 \\\\
   0.3 \\\\
   0.4 \\\\
   0 \\\\
   1 \\\\
   0 \\\\
   0 \\\\
  \end{matrix}
  \\right]</script>

<p>不存在目标的向量y：</p>

<script type="math/tex; mode=display">\\left[
 \\begin{matrix}
   0 \\\\
   ? \\\\
   ? \\\\
   ? \\\\
   ? \\\\
   ? \\\\
   ? \\\\
   ? \\\\
   ? \\\\
  \end{matrix}
  \\right]</script>

<p>问号，表示这些值不需要关心。（有个疑问：实际程序中应该用什么代表问号）</p>

<ol>
  <li>Loss function</li>
</ol>

<p>损失函数\(L(\hat y,y)\)可以用平方误差的形式，根据\(p_c\)是否为1分别计算：</p>

<ul>
  <li>如果\(\hat y\)里的\(p_c=1\)，则：
<script type="math/tex">L(\hat y,y)=(\hat y_1-y_1)^2+(\hat y_2-y_2)^2+\cdots+(\hat y_8-y_8)^2</script></li>
  <li>如果\(\hat y\)里的\(p_c=0\)则仅需要计算\(p_c\)的偏差，其他分量不需要考虑：
<script type="math/tex">L(\hat y,y)=(\hat y_1-y_1)^2</script></li>
</ul>

<p>上面举例时，每个分量都用平方误差计算，但实际可以每个分量使用不同的损失函数，比如\(p_c\)使用逻辑回归损失函数，分类标签使用softmax损失函数。bounding box四个分量使用平方误差。</p>

<h2 id="12--landmark-detection">1.2- Landmark Detection</h2>
<p>Localization给出了bounding box的坐标，更一般的，可以让卷积网络学习识别关键点的坐标，这些关键点就是<strong>特征点（Landmark）</strong>，这个过程即特征点检测（Landmark Detection）。</p>

<p>比如面部识别，可以输出的Landmark有：眼睛、嘴巴、鼻子等轮廓上关键点的坐标，根据需要识别的精度，调整需要输出的关键坐标点个数。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-11_22-02-48.jpg" alt="Xnip2018-07-11_22-02-48" /></p>

<p>又比如肢体动作识别：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-11_22-08-03.jpg" alt="Xnip2018-07-11_22-08-03" /></p>

<p>每个landmark用两个数字表示，比如\((l_{1x},l_{1y})\)表示第一个关键点的x和y坐标。</p>

<p>Landmark Detection的应用：面部识别、动作捕捉、AR。</p>

<p>需要注意的是，<strong>Landmark在所有样本里定义要保持一致</strong>，比如第1个landmark永远表示做眼外眼角的位置。</p>

<h2 id="13--object-detection">1.3- Object Detection</h2>

<ol>
  <li><strong>Closely cropped images</strong>
假如要做的Car Detection，首先用<strong>closely cropped images</strong>（即紧贴着对象裁剪的图片）训练一个卷积网络，可以识别closely cropped images是否是汽车图片。</li>
</ol>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-11_22-17-02.jpg" alt="Xnip2018-07-11_22-17-02" /></p>

<p>此卷积网络用于下面的sliding windows detection。</p>

<ol>
  <li><strong>Sliding windows detection</strong></li>
</ol>

<p>过程如下：</p>
<ul>
  <li>首先选取一个固定大小的矩形window，用这个window以一个步长在图片上从左到右，从上到下依次扫过。</li>
  <li>每次window扫过的区域，使用之前根据closely cropped images训练好的卷积网络，识别区域内是否有目标对象。</li>
  <li>不断增大window的大小，重复上面的操作。最后可能在某一个大小的window，在某个位置输出包含目标对象，这个位置的window就是目标对象的bounding box。</li>
</ul>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-11_22-23-50.jpg" alt="Xnip2018-07-11_22-23-50" /></p>

<p>Sliding windows的主要缺点：</p>
<ul>
  <li>计算量巨大，因为要用不同大小的window，对整个图片不同位置做检测。window的大小和步长的大小选择对计算量影响也很大。</li>
  <li>如果window选择的太大、步长太大，定位的精度也很差。</li>
</ul>

<p>计算量的问题，可以通过Convolutional Implementation of Sliding Windows解决。</p>

<h2 id="14--convolutional-implementation-of-sliding-windows">1.4- Convolutional Implementation of Sliding Windows</h2>

<ol>
  <li><strong>Turning FC layer into Convolutional layer</strong></li>
</ol>

<p>下图中上半部分，最后两层是FC layer，输出是softmax。下半部分是将<strong>FC layer解释为Conv layer</strong>的示意。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_08-26-14.jpg" alt="Xnip2018-07-12_08-26-14" /></p>

<p><strong>可以看出，当filter的大小和输入图片的大小一致，则ConvNet本质上就是FC</strong>。所以，这里所谓的转换成Conv Layer，只不过是观察和理解的角度不一样。</p>

<p><strong>但一旦将FC转换为Conv后，有个好处是，你就不再受限于FC了，比如一旦FC前面的矩阵维度扩充了，Conv后的维度也可以跟着扩充</strong>。比如上面的输入矩阵如果改成16x16，其他参数都不变，则如下：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_09-00-04.jpg" alt="Xnip2018-07-12_09-00-04" /></p>

<p>可以看出最后几层就变成了2x2的矩阵了，而FC是做不到的！下面将会用到这个特性。</p>

<ol>
  <li><strong>Convolutional Implementation of Sliding Windows</strong></li>
</ol>

<p>假如sliding window的Convnet架构如下（即用于检测closely cropped images中是否包含目标）：
输入一个14x14x3的图片，输出是否包含4种类型的目标。其中最后几个FC Layer是通过ConvNet实现的。另外图中的volume只画了正面，没有画channel维度。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_18-01-28.jpg" alt="Xnip2018-07-12_18-01-28" /></p>

<p>假如被测图片是一个16x16x3的图片（即要用slide window切分的，这里为了说明，维度比较小，实际上被测图片一般比slide window要大很多，黄色部分是比window大的区域），如果按步长2进行窗口滑动，一共可以滑动出4个窗口区域。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_18-10-06.jpg" alt="Xnip2018-07-12_18-10-06" /></p>

<p>如果使用Sliding windows detection，需要切分出4个图片，<strong>分别</strong>放到ConvNet里去检测。可以看出<strong>4个window截取的区域是有重合数据的，这造成了ConvNet的4次计算有重复计算量</strong>。而Convolutional Implementation of Sliding Windows则可以<strong>共享这部分重复计算</strong>，从而大幅降低整体计算量。</p>

<p>具体做法是，将被测图片直接输入到sliding window的Convnet中（相同的卷积网络和参数）：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_18-15-15.jpg" alt="Xnip2018-07-12_18-15-15" /></p>

<p><strong>由于FC层是用ConvNet实现的，所以FC层和最后的输出会变成多维（这里是2x2），而2x2的4个数据对应的就是被测图片被sliding window切分的4个区域的预测输出</strong>。</p>

<p>对此，我的理解是：</p>
<ul>
  <li><strong>卷积的计算，本身就是类似于slide window</strong>，不断的将filter以一个stride在原始图片上从左到右，从上到下滑动做卷积。而这个过程和单独slide好window交给Closely cropped images是一样的过程。</li>
  <li><strong>ConvNet是稀疏连接的</strong>，比如2x2的输出结果，<strong>左上角的一个输出值仅受输入图片左上角14x14的区域影响</strong>。</li>
  <li>由于上面的原因，<strong>一方面ConvNet帮助共享了计算，并同时参与计算，而不是像sliding window那样串行的计算，另一方面又达到了和sliding window的效果一致</strong>。</li>
</ul>

<p>下面是更大的输入图片的例子：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_18-25-36.jpg" alt="Xnip2018-07-12_18-25-36" /></p>

<p>但此方法还是有一个缺点：bounding box并不准确，毕竟bounding box是由sliding window决定的。</p>

<p>paper参考：<a href="https://arxiv.org/pdf/1312.6229.pdf">Sermanet et al., 2014, OverFeat: Integrated recognition, localization and detection using convolutional networks</a></p>

<h2 id="15--bounding-box-predictions">1.5- Bounding Box Predictions</h2>

<p>由于sliding window是固定选取的，所以识别出的bounding box可能和目标的形状实际不符，比如实际更宽或更高，或者stride比较大。一个较好的解决方法是：YOLO（You Only Look Once）算法。</p>

<ol>
  <li>YOLO的基本过程：
    <ul>
      <li>没有sliding window，而是将图片拆分为若干个grid（比如示例中拆分为3x3，共9个grid；实际操作会拆分更多）
 <img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_22-06-59.jpg" alt="Xnip2018-07-12_22-06-59" /></li>
      <li>对每个grid应用Classification with localization算法（前面章节提到的），训练数据和算法的输出包含了目标的bounding box坐标的向量，比如：
 <script type="math/tex">\\left[
  \\begin{matrix}
 p_c \\\\
 b_x \\\\
 b_y \\\\
 b_h \\\\
 b_w \\\\
 c_1 \\\\
 c_2 \\\\
 c_3 \\\\
\end{matrix}
\\right]</script></li>
      <li>一共输出9个向量（和grid个数一样）</li>
    </ul>
  </li>
  <li>关于bounding box的坐标：
    <ul>
      <li>目标是否存在于grid的标准是目标的<strong>中心点是否在grid内</strong>。</li>
      <li>bounding box的<strong>坐标是以grid为参考的，而非整个图片</strong>。即将一个grid的左上角设置为(0,0)，右下角设置为(1,1)。</li>
      <li>bounding box的中心点坐标\(b_x\)和\(b_y\)的值必须是0到1之间，但高度\(b_h\)和宽\(b_w\)可以是比1大，说明超过了grid（事实上不超过1也可能是在grid之外），换句话说，<strong>高度和宽度是按照物体的实际高度和宽度来算的，不受grid的限制</strong>。</li>
      <li>bounding box的大小不是固定的，而是直接由模型输出，模型可以以任何精度输出bounding box</li>
    </ul>
  </li>
  <li>YOLO算法是卷积实现</li>
</ol>

<p>已上面9x9个grid的例子，每个grid输出8维向量，则一共输出的是3x3x8的volume。可以设计一个这样的卷积网络：
100x100x3 ==&gt; CONV ==&gt; MaxPooling ==&gt; … ==&gt; 3x3x8</p>

<p>最后的3x3x8的volume，每一个1x1x8代表输入图片对应位置的grid的结果，包括是否含有目标，以及bounding box坐标和目标分类。
训练数据，也按照这个关系进行标记，放到上面的网络训练。</p>

<p>由于YOLO是卷积实现，并不需要根据grid拆分结果反复计算，因此计算量也会大幅减少。增加grid的数量，可降低两个物体在一个grid内的概率。</p>

<ol>
  <li>YOLO的优点：
    <ul>
      <li>和Classification with localization类似，直接输出bounding box，不受sliding window和步长的限制，更为精确。</li>
      <li>卷积实现，仅进行一次CNN正向计算，计算量降低。</li>
    </ul>
  </li>
</ol>

<p>我的理解：另外一个角度看，YOLO算法也颇有end-to-end learning的过程，中间并不像sliding window一样要做window的切分（这种切分更具有hand-engineering的意味）。而YOLO直接从原图片到输出在哪个grid以及具体的bounding box。</p>

<p>paper参考：<a href="https://arxiv.org/pdf/1506.02640.pdf">You Only Look Once: Unified, Real-Time Object Detection</a>
Andrew Ng建议：YOLO的paper是最难的paper之一，阅读要有心理准备。</p>

<h2 id="16--intersection-over-union">1.6- Intersection Over Union</h2>
<p>如何检验目标检测算法给出的bounding box的效果呢？比如下图红色是汽车实际的bounding box，紫色是算法给出的bounding box，如何量化这个效果呢？</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_22-23-42.jpg" alt="Xnip2018-07-12_22-23-42" /></p>

<p>一个很自然的想法是看两个bounding box的重合度，具体可以用交集和并集的比值评判。比如下图黄色阴影部分是两个bounding box的交集（即重合部分），而绿色部分是两个bounding box的并集。IoU（Intersection Over Union）即这两个部分面积之比：</p>

<script type="math/tex; mode=display">IoU = \frac{\text{Intersection}}{\text{Union}}</script>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-12_22-26-11.jpg" alt="Xnip2018-07-12_22-26-11" /></p>

<p>IoU 的值介于0到1之间，越接近1表示目标的定位越准确，IoU惯例上用&gt;=0.5表示“正确”定位的阈值，或根据情况设置这个阈值。</p>

<h2 id="17--non-max-suppression">1.7- Non-max Suppression</h2>

<p>目标检测遇到的一个常见问题是：同一个目标会在多个地方检测到。而Non-max Suppression可以保证一个物体只被检测到一次。</p>

<p>比如用YOLO算法，将下图划分为19x19个grid，图中有两辆车，绿色点和黄色点是实际的目标中心点，但与之相邻的几个grid可能也会报告自己的\(p_c\)值很大，找到了目标。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-13_08-11-28.jpg" alt="Xnip2018-07-13_08-11-28" /></p>

<p>最终检测的结果可能是如下多个bounding box（数字是检测出的\(p_c\)值）：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-13_08-15-27.jpg" alt="Xnip2018-07-13_08-15-27" /></p>

<p>很自然的想法是，将识别了<strong>同一个物体</strong>的grid，取\(p_c\)值最大的grid。想法虽然很对，但关键点是<strong>怎么知道哪些grid识别了同一个物体呢</strong>，如上图有两个汽车，怎么将\(p_c\)值是0.8和0.9的两个grid找出？Non-max Suppression的步骤是：</p>
<ol>
  <li>首先丢弃所有\(p_c\)值小于0.6（该阈值可调整）的grid。（\(p_c\)太小的，很可能是识别了一些边边角角，可以直接丢弃）</li>
  <li>对于所有剩余的grid
    <ul>
      <li>选取\(p_c\)值最大的grid A作为输出，即从剩余grid拿掉。</li>
      <li>丢弃所有与A的IoU大于等于0.5（该阈值可调整）的grid。（这些很可能是与A相邻的，识别了同一个对象）</li>
    </ul>
  </li>
  <li>重复第2步，直到没有剩余的grid为止。</li>
</ol>

<p>把这个算法应用到上图两量汽车的例子：</p>
<ol>
  <li>丢弃小于0.6的grid，这里没有要丢弃的</li>
  <li>选取最大的0.9的grid作为第一个目标的输出，并且将与其bounding box的IoU超过0.5的两个grid（即右边的两个0.6和0.7的）剔除。</li>
  <li>接下来再看剩余grid（只有左边0.7和0.8两个）里\(p_c\)值最大的grid，显然是0.8的grid，将其作为第二个目标的输出。0.7与其IoU超过0.5，被剔除。</li>
  <li>已无剩余的grid，并输出了两个目标的grid。</li>
</ol>

<p>non-max的意思就是输出最大可能性的grid，并抑制与其有重叠的非最大值grid</p>

<p>上述步骤只是对单个类别的目标检测，如果有多个类型的目标，则按每个类型分别做Non-max Suppression。</p>

<h2 id="18--anchor-boxes">1.8- Anchor Boxes</h2>
<p>到目前，我们的算法中一个grid只能检测一个物体，如果要想在一个grid里检测多个物体怎么办？比如下图，汽车和人的中心点是重合的。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-13_09-05-45.jpg" alt="Xnip2018-07-13_09-05-45" /></p>

<p>根据之前定义的输出向量y，算法在此gird输出目标类型的时候只能选择一种。</p>

<script type="math/tex; mode=display">\\left[
 \\begin{matrix}
   p_c \\\\
   b_x \\\\
   b_y \\\\
   b_h \\\\
   b_w \\\\
   c_1 \\\\
   c_2 \\\\
   c_3 \\\\
  \end{matrix}
  \\right]</script>

<p>Anchor Boxes的思想是，预定义两个形状不同的称作Anchor Box的矩形。 将要预测的两个对象分别关联到两个Anchor Box上。（实践中，会定义多个Anchor Box，这里只用2个作说明 ）</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-13_11-27-56.jpg" alt="Xnip2018-07-13_11-27-56" /></p>

<p>首先将输出的label定义为两个部分（上下两部分的元素和无Anchor box的定义是一样的）：
    <script type="math/tex">\\left[
     \\begin{matrix}
       p_c \\\\
       b_x \\\\
       b_y \\\\
       b_h \\\\
       b_w \\\\
       c_1 \\\\
       c_2 \\\\
       c_3 \\\\
       p_c \\\\
       b_x \\\\
       b_y \\\\
       b_h \\\\
       b_w \\\\
       c_1 \\\\
       c_2 \\\\
       c_3 \\\\
      \end{matrix}
      \\right]</script></p>

<p>其中，上半部分对应第一个Anchor Box，下半部分对应第二个Anchor Box。由于人的形状是高瘦的，更接近于第一个Anchor box，因此可以用label上半部分对行人是否存在编码；同样，汽车形状接近于第二个Anchor Box，因此用label下半部分编码。</p>

<p>算法对比：</p>
<ul>
  <li>原算法：训练图片中的每个目标，被分配到包含该物体中心点的grid里。</li>
  <li>Ancho Boxes：训练图片中的每个目标，被分配到包含该物体中心点的grid和与<strong>其bounding box有最大IoU的anchor box</strong>中。相当于增加了一个匹配维度，即（grid cell, anchor box）。</li>
  <li>本例中，y的维度从3x3x8变成了3x3x2x8。</li>
</ul>

<p>Anchor Box无法处理的情况：</p>
<ul>
  <li>如果设定了两个anchor box，但grid里面有3个物体，则也无法处理</li>
  <li>如果是两个物体的anchor box一样，也无法处理。</li>
</ul>

<p>遇到这些情况，只能根据默认的规则随便选一个物体了。</p>

<p>虽然我们用Anchor Box解决两个物体在同一个中心点的问题。但实践中grid切分的比较多，比如19x19个grid，出现两个物体在相同的中心点的概率并不大。除此之外，<strong>Anchor Box的作用之一是让你算法更专注训练某种特定物体</strong>，比如让某些输出单元专注于高瘦的行人，而另一些输出单元专注于训练宽长的汽车。</p>

<p>如何选择Anchor Box？可以手工选择，也可以用K-means算法聚类，根据聚类的结果选择anchor box。</p>

<h2 id="19--yolo-algorithm">1.9- YOLO Algorithm</h2>
<p>根据前面介绍的内容，我们可以组合成YOLO Algorithm。假设有如下问题用以说明YOLO算法：</p>

<p>我们需要识别图中的三类目标：1-行人 2-汽车 3-摩托车，使用3x3的grid划分，2个anchor box。则label y的维度是3x3x2x8，可表示为：</p>

<script type="math/tex; mode=display">\\left[
     \\begin{matrix}
       p_c \\\\
       b_x \\\\
       b_y \\\\
       b_h \\\\
       b_w \\\\
       c_1 \\\\
       c_2 \\\\
       c_3 \\\\
       p_c \\\\
       b_x \\\\
       b_y \\\\
       b_h \\\\
       b_w \\\\
       c_1 \\\\
       c_2 \\\\
       c_3 \\\\
      \end{matrix}
      \\right]</script>

<p>首先构建training set，人工查看3x3的grid，对每个grid给出标注值y：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-15_11-10-14.jpg" alt="Xnip2018-07-15_11-10-14" /></p>

<p>设计的ConvNet，输出volume则是3x3x16，通过这个ConvNet可以输出每个grid的预测值。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-15_11-13-00.jpg" alt="Xnip2018-07-15_11-13-00" /></p>

<p>然后通过Non-max Suppression，产生最后的预测值。</p>

<h2 id="110--region-proposals">1.10- Region Proposals</h2>
<p>前面介绍的sliding window或Convolutional Implementation of Sliding Windows，都有一个缺点：<strong>在一些空白的，明显没有物体的区域做检测</strong>。</p>

<p>R-CNN（Regions with CNNs）的思想是，通过Segmentation algorithm算法（即产生最右面的色块图）找出候选区域，只对有可能出现目标的色块进行检测，输出label和bounding box。相比sliding window的逐行逐列，减少了检测量。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-15_12-04-15.jpg" alt="Xnip2018-07-15_12-04-15" /></p>

<p>但<strong>R-CNN运算很慢</strong>，衍生出了改良算法：</p>
<ul>
  <li>Fast R-CNN：使用Convolutional Implementation of Sliding Windows去识别候选区域。</li>
  <li>Faster R-CNN：使用CNN做候选区域的识别。</li>
</ul>

<p>即便是Faster R-CNN，也比YOLO慢很多。但Andrew Ng认为，长远看来这是更有前途的算法。</p>

<p>paper参考：</p>
<ul>
  <li><a href="https://arxiv.org/pdf/1311.2524.pdf">R-CNN：Girshik et al., 2013. Rich feature hierarchies for accurate object detection and semantic segmentation</a></li>
  <li><a href="https://arxiv.org/pdf/1504.08083.pdf">Fast R-CNN：Girshik, 2015. Fast R-CNN</a></li>
  <li><a href="https://arxiv.org/pdf/1506.01497v3.pdf">Faster R-CNN：Ren et al., 2016. Faster R-CNN: Towards real-time object detection with region proposal networks</a></li>
</ul>
