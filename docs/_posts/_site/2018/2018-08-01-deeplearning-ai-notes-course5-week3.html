<script type="text/javascript" async="" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p>本周是deeplearning.ai系列的最后一节课，以机器翻译为例，介绍了seq2seq模型，扩展分析了beam search、bleu score和attention model。最后简要介绍了语音识别和触发词检测。</p>

<h1 id="1--various-sequence-to-sequence-architectures">1- Various sequence to sequence architectures</h1>
<h2 id="11--basic-models">1.1- Basic Models</h2>

<p>这周将学习<strong>序列到序列（seq2seq, sequence to sequence）模型</strong>，seq2seq在序列到序列的转换方面非常有用，尤其是机器翻译和语音识别。</p>

<p>seq2seq模型主要来源于如下两篇论文：</p>

<ul>
  <li><a href="https://arxiv.org/pdf/1409.3215.pdf">Sutskever et al., 2014. Sequence to sequence learning with neural networks</a></li>
  <li><a href="https://arxiv.org/pdf/1406.1078.pdf">Cho et al., 2014. Learning phrase representations using RNN encoder-decoder for statistical machine translation</a></li>
</ul>

<p>我们先从最基本的模型开始。假设我们有一个机器翻译的任务，将法语翻译成英语，比如：</p>

<blockquote>
  <p>法语：Jane  visite  l’Afrique  en  septembre.
英语：Jane  is  visiting  Africa  in  September.</p>
</blockquote>

<p>通常，我们用\(x\)和\(y\)将输入和输出表示为序列，如下：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-30_20-03-40.jpg" alt="Xnip2018-07-30_20-03-40" /></p>

<p><strong>我们的目标就是，训练一个神经网络，输入序列\(x\)，输出序列\(y\)。</strong> 首先，我们先创建一个称为<strong>编码器（encoder）</strong> 的RNN（具体可以是GRU或LSTM），并把法语序列输入到该网络。当encoder将所有单词都“吞下”后，在最后的时间步的状态输入到一个称为<strong>解码器（decoder）</strong> 的RNN，该RNN输出翻译后的英语，如下图：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-30_21-04-14.jpg" alt="Xnip2018-07-30_21-04-14" /></p>

<p>在给出足够的法语和英语对应文本的情况下，这个模型的效果相当不错。该模型简单的使用一个编码网络，将输入的法语编码，然后用一个解码网络生成对应的英语翻译。</p>

<p>和上面非常类似的架构，同时也应用于图像描述（image captioning）。比如，下面输入一张图片，给出图片的描述（即看图说话）：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-30_20-31-42.jpg" alt="Xnip2018-07-30_20-31-42" /></p>

<p>结合之前我们学习的CNN网络，我们先用CNN对图片进行编码，比如用预训练好的AlexNet得到4096维的编码，然后将编码输入到RNN作为解码器，输出对图像的描述：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-30_21-03-02.jpg" alt="Xnip2018-07-30_21-03-02" /></p>

<p>上面的图片描述模型，几乎被多篇论文同时提出：</p>

<ul>
  <li><a href="https://arxiv.org/pdf/1412.6632.pdf">Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks</a></li>
  <li><a href="https://arxiv.org/pdf/1411.4555.pdf">Vinyals et. al., 2014. Show and tell: Neural image caption generator</a></li>
  <li><a href="https://arxiv.org/pdf/1412.2306.pdf">Karpathy and Li, 2015. Deep visual-semantic alignments for generating image descriptions</a></li>
</ul>

<p>上面的基本seq2seq模型，和之前讨论的使用语言模型产生新文本还有差别。<strong>主要在于，我们并不下午随机的选择翻译或图片描述，而是希望选择可能性最大的</strong>。接下来我们将介绍如何做到。</p>

<h2 id="12--picking-the-most-likely-sentence">1.2- Picking the most likely sentence</h2>

<p>seq2seq翻译模型和先前讨论的语言模型既有些相似，也有很大不同。<strong>我们可以将机器翻译看做构建一个条件语言模型（conditional language model）</strong>。</p>

<p>下图是两个模型的对比，我们可以看出机器翻译模型的decoder部分和语言模型十分类似，唯一差别在于，机器翻译的最初输入状态来源于encoder，而语言模型的输入固定为\(a^{&lt;0&gt;}\)。</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-30_21-08-46.jpg" alt="Xnip2018-07-30_21-08-46" /></p>

<p>语言模型计算的是概率：</p>

<script type="math/tex; mode=display">% <![CDATA[
P(y^{\<1>},...,y^{\<T_y>}) %]]></script>

<p>而机器翻译模型，计算的是条件概率（这也是其称为条件语言模型的原因）：</p>

<script type="math/tex; mode=display">% <![CDATA[
P(y^{\<1>},...,y^{\<T_y>}| x^{\<1>},...,x^{\<T_x>}) %]]></script>

<p>解码器随机采样，可能导致翻译结果时好时坏，因此我们不能随机的抽样出翻译结果，<strong>而是找到令上面条件概率最大化的翻译</strong>，即：</p>

<script type="math/tex; mode=display">% <![CDATA[
\text{arg} \ \text{max}\_{y^{\<1>}, ..., y^{\<T_y>}}P(y^{\<1>}, ..., y^{\<T_y>} | x) %]]></script>

<p>因此在开发机器学习系统时，你要做的事情之一就是使用算法<strong>求出使得上面条件概率最大的\(y\)。其中最常用的算法之一是集束搜索（Beam Search）</strong>。</p>

<p>在学习Beam Search之前，我们先看一下为什么不用贪心搜索（greedy search），贪心搜索只是最大化解码器当期时间步的条件概率，而我们希望的是整个语句的条件概率最大。贪心算法很容易陷入次优解，所以不适合。</p>

<p>与greedy search对应的是另外一个极端：对全体空间搜索。比如翻译的语句长度是10，词汇表是10000，则需要搜索\(10000^{10}\)个句子，这个搜索量和让猴子敲出莎士比亚文集没什么区别，显然是不现实的。</p>

<h2 id="13--beam-search">1.3- Beam Search</h2>

<p>我们还以上面的法语翻译为例，讲解Beam Search的步骤：</p>
<ul>
  <li>首先看第一个单词的条件概率\(P(\hat y^{&lt;1&gt;}|x)\)，并<strong>选择概率最大的3个单词作为候选</strong>，这里个数3称为<strong>集束宽（beam width）</strong>，代表解码器中每个时间步候选的单词个数，记作\(B\)。具体上面的例子，可能选到了3个候选单词是：in, jane, september。
  <img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-31_09-29-29.jpg" alt="Xnip2018-07-31_09-29-29" /></li>
  <li>然后分别考虑第1个单词是上面3个候选单词之一的条件下，第2个单词的概率\(P(\hat y^{&lt;2&gt;}|x, \hat y^{&lt;1&gt;})\)。比如第一个单词是in的概率\(P(\hat y^{&lt;2&gt;}|x, \text{‘in’})\)。然后再选出构成前两个单词概率最大的3个组合，其概率计算为：
  <script type="math/tex">% <![CDATA[
P(\hat y^{\<1>}, \hat y^{\<2>}|x) = P(\hat y^{\<1>}|x) P(\hat y^{\<2>}|x, \hat y^{\<1>}) %]]></script>
  由于\(B=3\)，因此我们在这一步会考虑3x10000种组合（第一步对应3个单词，第二步对应词汇表10000个单词），并选择其中概率最大的3个。最终可能的候选是：in september, jane is, jane visits（第一个单词是september的情况已经被剔除了）。我们将30000的搜索范围，收缩为3个候选。
  <img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-31_09-30-46.jpg" alt="Xnip2018-07-31_09-30-46" /></li>
  <li>第3步和第2步类似，可能继续得到3个候选：in september jane, jane is visiting, jane visits africa
  <img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-31_09-26-34.jpg" alt="Xnip2018-07-31_09-26-34" /></li>
</ul>

<p>特别的，<strong>如果\(B=1\)，beam search就退化为了贪心搜索</strong>。</p>

<h2 id="14--refinements-to-beam-search">1.4- Refinements to Beam Search</h2>

<p>下面介绍一些对Beam search算法的优化。</p>

<ol>
  <li>长度归一化（length normalization）</li>
</ol>

<p>我们知道Beam Search的优化目标是：
<script type="math/tex">% <![CDATA[
\text{arg} \ \text{max}\_{y^{\<1>}, ..., y^{\<T_y>}}P(y^{\<1>}, ..., y^{\<T_y>} | x) %]]></script></p>

<p>展开为条件概率的乘积：
<script type="math/tex">% <![CDATA[
\text{arg max} \prod^{T_y}\_{t=1} P( y^{\<t>} | x, y^{\<1>}, ..., y^{\<t-1>}) %]]></script></p>

<p>实践中，每个因子通常是远小于1的概率，经过一连串的乘积会导致结果为一个很小很小的数字，导致<strong>下溢（underflow）</strong>，即计算机没法精确表示。因此实际操作中，为<strong>将优化目标取对数</strong>，规避下溢问题。我知道对数函数是单调递增的，因此两个优化目标等价，即：</p>

<script type="math/tex; mode=display">% <![CDATA[
\text{arg max} \sum^{T_y}\_{t=1} \log P( y^{\<t>} | x,  y^{\<1>}, ...,  y^{\<t-1>}) %]]></script>

<p>观察上面的优化目标，可以发现优化目标倾向于选择长度短的输出序列，因为每个条件概率都小于1，因此序列越短总的概率越大，这并不是我们希望的。因此我们再增加一个系数对太短的语句惩罚，简单点就用输出序列的总长度做一下平均：</p>

<script type="math/tex; mode=display">% <![CDATA[
\text{arg max} \frac{1}{T_y} \sum^{T_y}\_{t=1} \log P( y^{\<t>} | x,  y^{\<1>}, ...,  y^{\<t-1>}) %]]></script>

<p>实践中，一般我们会再加一个更柔和的处理方法，即\(T_y\)上加上指数\(\alpha\)：</p>

<script type="math/tex; mode=display">% <![CDATA[
\text{arg max} \frac{1}{T_y^\alpha} \sum^{T_y}\_{t=1} \log P( y^{\<t>} | x,  y^{\<1>}, ...,  y^{\<t-1>}) %]]></script>

<p>如果\(\alpha=1\)，就完全用长度来归一化；如果\(\alpha=0\)，就是没有做归一化。一般会选择一个中间值。<strong>\(\alpha\)也是算法的一个超参</strong>，需要不断调整来得到最好的结果，并没有理论性验证。</p>

<p>上式也称为<strong>归一化对数概率目标（normalized log probability objective）</strong>或<strong>归一化对数似然目标（normalized log likelihood objective）</strong>。</p>

<p>总结一下如何运行beam search算法。运行beam search，会得到一系列长度的翻译语句，假如限制输出语句最长为30个单词，则得到长度从1到30的语句的概率。每个长度都会保留\(B\)个概率最高的语句。然后针对这些<strong>所有可能的输出语句</strong>，用上式给他们打分，取得分最高的一个语句作为翻译结果。</p>

<ol>
  <li>如何选择参数\(B\)</li>
</ol>

<p>很显然，\(B\)越大，取得的结果越良好，但计算量和内存需求也更大。</p>

<p>前面的讲解汇总\(B\)取3是比较小的，在产品中，经常可以看到\(B\)取10的情况。\(B\)的大小也是取决于应用场景的，比如论文中会看到\(B\)取值1000或者3000，主要是为了压榨出全部性能。</p>

<p>但\(B\)的提升的边际效益会递减，从1提升到3或者10的收益，要比从1000提升到3000的收益大得多。</p>

<p>另外，需要明白的是beam search和计算机中的其他搜索算法，如广度优先搜索（BFS）、深度优先搜索（DFS）不一样，<strong>它们都是精确的搜索算法</strong>。而<strong>beam search是一个近似搜索算法</strong>，并不保证搜索到实际的最大值。</p>

<h2 id="15--error-analysis-in-beam-search">1.5- Error analysis in beam search</h2>

<p>我们知道beam search是一个模糊的启发式搜索算法，并不保证搜索到实际的最大值，因此就需要对beam search的结果进行错误分析。我们需要<strong>分析错误的原因是RNN模型还是beam search</strong>，来指导我们的模型优化。</p>

<p>下面仍用法语翻译的例子，我们将人类翻译的结果记为\(y^*\)，算法翻译的结果记为\(\hat y\)：</p>

<p><img src="https://cdn.imshuai.com/images/2018/07/Xnip2018-07-31_21-35-25.jpg" alt="Xnip2018-07-31_21-35-25" /></p>

<p>虽然对RNN增加训练样本或者增加beam search的参数\(B\)对翻译结果总是没有坏处的，但我们要知道瓶颈在哪，才能有的放矢。</p>

<table>
  <tbody>
    <tr>
      <td>方法很简单，直接用RNN去计算人类翻译和算法翻译的概率，即比较\(P(y^*</td>
      <td>x)\)和\(P(\hat y</td>
      <td>x)\)做判断：</td>
    </tr>
    <tr>
      <td>* 如果\(P(y^*</td>
      <td>x) &gt; P(\hat y</td>
      <td>x)\)，说明beam search效果不好，没有搜索到概率更大的翻译语句。</td>
    </tr>
    <tr>
      <td>* 如果\(P(y^*</td>
      <td>x) \le P(\hat y</td>
      <td>x)\)，说明RNN效果不好，较好的翻译语句没有得到更大的概率。</td>
    </tr>
  </tbody>
</table>

<p>基于上面的原则，我们做一个表格，对一定数量的错误案例进行分析，<strong>统计RNN和beam search出错的占比</strong>，得到模型的瓶颈在哪里，然后针对其优化。</p>

<p>做一个错误的分析表，选取一定的数量看一看。</p>

<h2 id="16--bleu-score">1.6- Bleu Score</h2>

<p>TODO:
衡量机器翻译结果和人工翻译结果的相似性（重合性）
如果只翻译出一个单词呢？
这一集的视频太水了，都懒得剪辑了。</p>

<p>单值评价</p>

<h2 id="17--attention-model-intuition">1.7- Attention Model Intuition</h2>

<p><strong>注意力模型（Attention Model）</strong> 是深度学习领域最有影响力的思想之一。</p>

<ol>
  <li>长序列问题</li>
</ol>

<p>之前我们使用的RNN模型，都是encoder完整输入所有待翻译的序列后，decoder再输出翻译后的序列。而面对长句子时，人类翻译者并不会一次性阅读完整个句子才翻译，而是拆分为一小段一小段的，逐段翻译，因为人们很难一次性记住很长的句子。</p>

<p>我们发现，随着句子的增长，Bleu Score开始降低：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_08-08-55.jpg" alt="Xnip2018-08-01_08-08-55" /></p>

<ol>
  <li>Attention model</li>
</ol>

<p>使用注意力模型可以显著解决这个问题，attention model受人类翻译的启发，<strong>我们并不希望神经网络每次记忆很长的文字，而是每次处理一段文字，这样在处理长句的情况下Bleu Score不降低</strong>。</p>

<p>（我有个疑问，为什么一定要和人一样，人是因为有记忆限制的缺陷。如果不用注意力模型，翻译效果会不会比人更好？）</p>

<p>首先，我们使用一个BRNN作为编码器，BRNN输出每个单词丰富的特征：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_08-26-52.jpg" alt="Xnip2018-08-01_08-26-52" /></p>

<p>然后我们用另外一个单向RNN作为解码器，为了避免两个RNN的激活函数混淆，此处解码器RNN用\(s\)表示激活函数。</p>

<p>我们的问题是，解码器的每一个时间步应该查看被翻译语句的哪些部分，即比如\(s^&lt;1&gt;\)应考虑编码器哪些输出。在注意力模型里，我们使用<strong>注意力权重（attention weights）</strong> 来表示对句子每个部分的权重，这个权重记为\(\alpha^{&lt;t,t’&gt;}\)，其中上标含义是，解码器的第\(t\)个时间步，对编码器第\(t’\)个输出的注意力权重。我们用\(c\)表示编码器激活函数在注意力权重加权后的结果，将\(c\)输入到解码器用来生成翻译语句，如下图：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_18-22-35.jpg" alt="Xnip2018-08-01_18-22-35" /></p>

<p>第二个输出也是类似，激活函数\(s^&lt;2&gt;\)的输入来自注意力权重加权的新结果\(c\)，当然同时会将上一个时间步输出的翻译结果也加入；以此类推，如下图：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_19-59-46.jpg" alt="Xnip2018-08-01_19-59-46" /></p>

<p>TODO: 注意力模型的思想主要来源论文：[Bahdanau et. al., 2014. Neural machine translation by jointly learning to align and translate]</p>

<h2 id="18--attention-model">1.8- Attention Model</h2>

<p>上一节介绍了注意力模型的Intuition，现在我们形式化定义。模型和上一节一样，下面是一个BRNN的编码器，上面是一个多对多的RNN解码器，如下图：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_20-38-45.jpg" alt="Xnip2018-08-01_20-38-45" /></p>

<p>其中每一个输出，单独拿出来如下：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_20-54-58.jpg" alt="Xnip2018-08-01_20-54-58" /></p>

<p>我们把编码器BRNN的每个单元的前向和后向激活函数分别记为\(\overrightarrow a^{&lt;t’&gt;}\)和\(\overleftarrow a^{&lt;t’&gt;}\)，为了便于说明，我们合并记为\(a^{&lt;t’&gt;}\)，即：</p>

<script type="math/tex; mode=display">% <![CDATA[
a^{\<t'>} = (\overrightarrow a^{\<t'>}, \overleftarrow a^{\<t'>}) %]]></script>

<p>对解码器的第\(t\)个时间步，其输入来自编码器的注意力加权\(c\)，以及前一个时间步的激活函数\(s^{&lt;t-1&gt;}\)，输出\(y^{&lt;t-1&gt;}\)。其中\(c\)的计算为：</p>

<script type="math/tex; mode=display">% <![CDATA[
c^{\<t>} = \sum_{t'}\alpha^{\<t,t'>}  a^{\<t'>} %]]></script>

<p>显然注意力权重\(\alpha\)要满足大于0，并且和为1：</p>

<script type="math/tex; mode=display">% <![CDATA[
\sum_{t'}\alpha^{\<t,t'>} = 1 %]]></script>

<p><strong>\(\alpha^{&lt;t,t’&gt;}\)，即代表了\(y^{&lt;t&gt;}\)对\(a^{&lt;t’&gt;}\)的注意力大小</strong>。这个权重的计算如下：</p>

<script type="math/tex; mode=display">% <![CDATA[
a^{\<t,t'>} = \frac{\text{exp}(e^{\<t,t'>})}{\sum^{T_x}\_{t'=1} \text{exp}(e^{\<t,t'>})} %]]></script>

<p>其中\(e\)的计算，使用一个小神经网络计算，如下：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_20-52-25.jpg" alt="Xnip2018-08-01_20-52-25" /></p>

<h1 id="2--speech-recognition---audio-data">2- Speech recognition - Audio data</h1>
<h2 id="21--speech-recognition">2.1- Speech recognition</h2>
<p>语音识别问题，就是将一段音频片段，自动转换为文字，seq2seq模型也可以应用于此。</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_21-01-06.jpg" alt="Xnip2018-08-01_21-01-06" /></p>

<p>语音识别问题，在早期是通过音位（phonemes）来构建的，需要手工工程设计的基本单元（hand-engineered basic units of cells）。我们会将单词拆分为基本的音位，如the拆分为th和e的音，而quick拆分为k w i k四个音。语音学家认为用户音位表示法（phonemes representations）是做语音识别的好办法。</p>

<p>但在end-to-end的深度学习模型中，已经没有必要人工去分析音位了。通过大量的文本语音数据集，运用深度学习算法大大推进了语音识别的进程。最好的商业系统，可能会使用长达10万小时的音频训练。</p>

<p>当然，我们可以使用attention model训练语音识别：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_21-14-21.jpg" alt="Xnip2018-08-01_21-14-21" /></p>

<p>还有一种方法效果非常不错，它使用了CTC（Connectionist Temporal Classification）损失函数。下面简要介绍：</p>

<p>假如我们要识别的语音对应的内容是”the quick brown fox”，</p>

<p>我们将用一个输入和输出大小相同的RNN（这里使用单向RNN说明，实践中会用BRNN）：</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_21-36-38.jpg" alt="Xnip2018-08-01_21-36-38" /></p>

<p>需要注意的是，输入的时间步通常会比输出的时间步大很多。举个例子，10秒钟的音频，如果用100Hz采样，则每秒就会产生100个样本，于是10秒的音频，会产生1000个输入，但输出的字符肯定没有这么多。CTC的做法是，允许RNN产生类似如下的输出：</p>

<blockquote>
  <p>ttt_h_eeee_ _ _ &lt;space&gt; _ _ _qqq_ _ _……</p>
</blockquote>

<p>其中_表示空白符，&lt;space&gt;表示空格。将空白符分隔的字符折叠起来，然后去掉空白符，则得到了输出文本：</p>
<blockquote>
  <p>the quick brown fox</p>
</blockquote>

<p>这样就允许整个网络有1000个有重复的字母的输出，但最终仍能得到短得多的文本输出。</p>

<p>paper参考：<a href="http://people.idsia.ch/~santiago/papers/icml2006.pdf">Graves et al., 2006. Connectionist Temporal Classification: Labeling unsegmented sequence data with recurrent neural networks</a></p>

<h2 id="22--trigger-word-detection">2.2- Trigger Word Detection</h2>

<p>相比通用的语音识别系统需要大量训练数据，触发词检测（Trigger Word Detection）要简单得多，使用较小的数据量即可训练。</p>

<p>触发检测系统，主要用于一些设备的唤醒，比如苹果的Siri、智能音箱等。下面介绍一种实现算法。</p>

<p>现在有一个这样的RNN结构，我们要做的就是把一个音频片段计算出它的声谱图特征（spectrogram features），得到特征向量\(x^{&lt;1&gt;}, x^{&lt;2&gt;}, x^{&lt;3&gt;}, …\)，然后输入到RNN中，最后要做的就是定义目标标签\(y\)。假如某一点是出现了触发词，那么在这之前的标签都设置为0，而这一点的设置为1；如果过了一段时间，触发词再次被说了一次，再次把这个点设置为1。</p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_21-54-41.jpg" alt="Xnip2018-08-01_21-54-41" /></p>

<p><img src="https://cdn.imshuai.com/images/2018/08/Xnip2018-08-01_21-53-37.jpg" alt="Xnip2018-08-01_21-53-37" /></p>

<p>上面的算法有一个明显的缺点是，构建了一个<strong>很不平衡的训练集</strong>，即0比1要多得多。但有一个简单粗暴的办法可以解决，是的训练更容易。我们在一个时间步上输出1后，在变回0之前多输出几次1，或者在固定的一段时间内输出多个1.</p>

<h1 id="3--conclusion">3- Conclusion</h1>
<h2 id="31--conclusion-and-thank-you">3.1- Conclusion and thank you</h2>
<p>deeplearning.ai课程结束了！与开篇的AI is the new electricity的口号相呼应，Andrew Ng激励我们：</p>

<blockquote>
  <p>Deep learning is a super power! If that isn’t a superpower I don’t know what is.</p>
</blockquote>

<p><img src="https://cdn.imshuai.com/images/2018/08/DL-IS-SUPERPOWER.jpg" alt="" /></p>
